{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangsae91/project_so/blob/master/exploration/221025/%5BExp_12%5D20221025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "302ZdfCKLlhI"
      },
      "outputs": [],
      "source": [
        "# google colab전용\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujMb2MtyLrVQ"
      },
      "source": [
        "# exploration 12번째 과제\n",
        "@ 황한용(3기/쏘카)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdrfR8Y3L0ry"
      },
      "source": [
        "## 라이브러리 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWLP8EFSLqqp"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from summa.summarizer import summarize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import AdditiveAttention, Concatenate, Dense, Embedding, Input, LSTM, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bs4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qC6hZWKL7x3"
      },
      "source": [
        "## 상수선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcIjEEgnMD75"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/data/news\" # 데이터 기본경로\n",
        "DATA_PATH = BASE_PATH + \"/news_summary_more.csv\" # 기사데이터\n",
        "CONTRACTIONS = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                \"you're\": \"you are\", \"you've\": \"you have\"} #  텍스트 정규화 사전\n",
        "\n",
        "SOS_TOKEN = \"sostoken\" # 문장 시작토큰\n",
        "EOS_TOKEN = \"eostoken\" # 문장 끝 토큰\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_SIZE = 256\n",
        "\n",
        "pad_seq_kwargs = {\n",
        "    \"value\":None # 추후 추가예정\n",
        "    , \"padding\":\"post\"\n",
        "    , \"maxlen\":None # 추후 추가예정\n",
        "}\n",
        "\n",
        "HIDDEN_STATE_NUM = 64 # hidden state의 노드수\n",
        "\n",
        "fit_kwargs = {\n",
        "    \"epochs\":50 # epoch 횟수\n",
        "    , \"batch_size\":HIDDEN_STATE_NUM\n",
        "    , \"callbacks\":EarlyStopping(monitor=\"val_acc\", patience=3)\n",
        "    , \"shuffle\" : True # epoch당 셔플을 할지의 여부\n",
        "    ,\"validation_data\": None # 추후 추가예정\n",
        "    , \"verbose\":1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX9YXWkpz7Rb"
      },
      "source": [
        "## 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M-r4yxnz7Rb"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(df:pd.DataFrame, col_name:str, remove_stopwords:bool=True) -> None:\n",
        "    \"\"\"\n",
        "    영어 문장의 내용을 전처리하는 함수\\n\n",
        "    처리전의 내용은 스크렙을 통해 가저온 기사로 가정하고 데이터를 처리한다.\n",
        "    처리과정은 아래의 순서에 따른다.\n",
        "    - 텍스트 소문자화\n",
        "    - 양 옆의 필요없는 whitespace, 개행문자 제거\n",
        "    - \\<br>, \\<a href \\= ...\\> 등의 html 태그 제거\n",
        "    - 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
        "    - 쌍따옴표(\") 제거\n",
        "    - 약어 정규화(`CONSTRUCTIONS`상수 참고)\n",
        "    - 소유격 제거. Ex) roland's -> roland\n",
        "    - 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
        "    - m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
        "    - 불용어 제거/미제거\n",
        "    - 양 옆의 필요없는 whitespace, 개행문자 제거\n",
        "\n",
        "    Examples\n",
        "    ----------\n",
        "    >>> df = pd.DataFrame(data=[\n",
        "    >>> [\n",
        "    >>>     '''Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, \n",
        "    >>>     was a Sr Systems Engineer at Infosys with almost 5 years of work experience.'''\n",
        "    >>>     '''Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year.'''\n",
        "    >>> ],[\n",
        "    >>>     '''upGrad learner switches to career in ML & Al with 90% salary hike''',\n",
        "    >>>     '''Delhi techie wins free food from Swiggy for one year on CRED'''\n",
        "    >>> ]\n",
        "    >>> ], columns=['text', 'summary'])\n",
        "    >>> preprocess_sentence(df, 'text')\n",
        "    >>> print(df['text'])\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        영어로 써있는 문장이 들어가있는 데이터프레임\n",
        "    col_name : str\n",
        "        전처리할 컬럼의 이름\n",
        "    remove_stopwords : bool, default = True\n",
        "        불용어 삭제여부\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    \"\"\"\n",
        "    stopwords_eng_dict=set(stopwords.words('english'))\n",
        "    df[col_name] = df[col_name].str.lower()\n",
        "    df[col_name] = df[col_name].str.strip()\n",
        "    df[col_name] = BeautifulSoup(\"\\n\".join(df[col_name].to_list()), \"lxml\").text.split(\"\\n\")\n",
        "    df[col_name] = df[col_name].replace(r\"\\([^)]*\\)\", \"\", regex=True)\n",
        "    df[col_name] = df[col_name].replace(r\"'s\\b\",\"\", regex=True)\n",
        "    df[col_name] = [\n",
        "                     \" \".join([\n",
        "                         CONTRACTIONS.get(w, w)\n",
        "                         for w in l\n",
        "                         ]) for l in df[col_name].str.split()\n",
        "                    ]\n",
        "    df[col_name] = df[col_name].replace(\"[^a-zA-Z]\", \" \", regex=True)\n",
        "    df[col_name] = df[col_name].replace(\"[m]{2,}\", \"mm\", regex=True)\n",
        "    if remove_stopwords:\n",
        "        df[col_name] = [\n",
        "                        \" \".join([\n",
        "                                w for w in l\n",
        "                                if (not w in stopwords_eng_dict\n",
        "                                and len(w) > 1)\n",
        "                            ]) for l in df[col_name].str.split()\n",
        "                        ]\n",
        "    else:\n",
        "        df[col_name] = [\n",
        "                        \" \".join([\n",
        "                                w for w in l\n",
        "                                if len(w) > 1\n",
        "                            ]) for l in df[col_name].str.split()\n",
        "                        ]\n",
        "    df[col_name] = df[col_name].str.strip()\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s.split()) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print(f\"전체 샘플 중 길이가 {max_len} 이하인 샘플의 비율: {cnt / len(nested_list)}\")\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "     # <SOS>에 해당하는 토큰 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_word_to_index[SOS_TOKEN]\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = tar_index_to_word[sampled_token_index]\n",
        "\n",
        "        if (sampled_token!=EOS_TOKEN):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "        if (sampled_token == EOS_TOKEN  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 상태를 업데이트 합니다.\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2text(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if (i!=0):\n",
        "            temp = temp + src_index_to_word[i]+' '\n",
        "    return temp\n",
        "\n",
        "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2summary(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if ((i!=0 and i!=tar_word_to_index[SOS_TOKEN]) and i!=tar_word_to_index['eostoken']):\n",
        "            temp = temp + tar_index_to_word[i] + ' '\n",
        "    return temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKFhG7BTz7Rc"
      },
      "source": [
        "속도의 문제로 인하여 소스코드를 변경하였다.<br>\n",
        "불용어 제거까지 적용시 늦어도 6초내외로 걸린다.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5DV1kd8MPes"
      },
      "source": [
        "## 메인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7XCWajZ0XIj"
      },
      "source": [
        "### 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwkvcfr7MPsm"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(DATA_PATH, encoding='iso-8859-1')\n",
        "data = data[['text','headlines']] # 필요데이터 select\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXm0KjWe845_"
      },
      "source": [
        "필요한 기사와 요약만 출력한다.<br>\n",
        "앞으로 `기사`는 `data['text']`, `요약`은 `data['headlines']`로 언급하겠다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgD5hvhwz7Rd"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates(subset = ['text'], inplace=True)\n",
        "data.dropna(axis=0, inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts1qandzz7Rd"
      },
      "source": [
        "기사의 중복값을 제거하며, `null`인 값을 가지고 있는 데이터를 row기준으로 삭제한다.<br>\n",
        "삭제후에는 다시 인덱스번호를 부여하여 인덱스 안에 빈값이 없도록 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "971TB3kw845_"
      },
      "outputs": [],
      "source": [
        "preprocess_sentence(data,'text')\n",
        "preprocess_sentence(data,'headlines',False)\n",
        "data.replace(\"\", np.nan, inplace=True)\n",
        "data.dropna(axis=0, inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajtkCZfqI5ew"
      },
      "source": [
        "`기사`는 불용어 제거, `요약`은 불용어 제거하지 않은 상태로 전처리를 하며<br>\n",
        "전처리로 인하여 인하여 생긴 빈 문자열은 제거한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX-uCKCAK1FQ"
      },
      "outputs": [],
      "source": [
        "text_max_len = 38\n",
        "headlines_max_len = 10\n",
        "\n",
        "text_len = [len(s.split()) for s in data['text']]\n",
        "headlines_len = [len(s.split()) for s in data['headlines']]\n",
        "\n",
        "print(f\"텍스트의 최소 길이 : {np.min(text_len)}\")\n",
        "print(f\"텍스트의 최대 길이 : {np.max(text_len)}\")\n",
        "print(f\"텍스트의 평균 길이 : {np.mean(text_len)}\")\n",
        "print(f\"요약의 최소 길이 : {np.min(headlines_len)}\")\n",
        "print(f\"요약의 최대 길이 : {np.max(headlines_len)}\")\n",
        "print(f\"요약의 평균 길이 : {np.mean(headlines_len)}\")\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.boxplot(text_len)\n",
        "plt.title('text')\n",
        "plt.subplot(1,2,2)\n",
        "plt.boxplot(headlines_len)\n",
        "plt.title('headlines')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.title('text')\n",
        "plt.hist(text_len, bins = 40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "\n",
        "plt.title('headlines')\n",
        "plt.hist(headlines_len, bins = 40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "\n",
        "below_threshold_len(text_max_len, data['text'])\n",
        "below_threshold_len(headlines_max_len,  data['headlines'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9kNM2TiK1RF"
      },
      "source": [
        "기사와 요약의 최소, 최대, 평균 길이와 길이 분포를 시각화하였다.<br>\n",
        "전체 데이터의 약 80% 정도가 포함되는 길이를 선정하였다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHh2B0mMz7Rd"
      },
      "outputs": [],
      "source": [
        "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
        "data = data[data['headlines'].apply(lambda x: len(x.split()) <= headlines_max_len)]\n",
        "data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPiUclN1z7Re"
      },
      "source": [
        "전체 데이터의 약 80% 정도가 포함되는 길이 이외의 데이터 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XjNp_mJz7Re"
      },
      "outputs": [],
      "source": [
        "data['decoder_input'] = SOS_TOKEN + \" \" + data['headlines']\n",
        "data['decoder_target'] = data['headlines'] + \" \" +EOS_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJSFhk6xz7Re"
      },
      "source": [
        "디코더의 입력값은 `SOS`를 앞에, 타겟값은 `EOS`를 붙여 데이터를 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZirTf13z7Re"
      },
      "outputs": [],
      "source": [
        "encoder_input = np.array(data['text']) # 인코더의 입력\n",
        "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
        "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "352hyz30z7Re"
      },
      "source": [
        "인코더의 입력, 디코더의 입력, 타겟 값을 `np.array` 형식으로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhp1hCfRz7Re"
      },
      "outputs": [],
      "source": [
        "# index shuffle\n",
        "shuffle_idx = data.index.to_list()\n",
        "news_valid_len = int(len(shuffle_idx) * 0.2)\n",
        "\n",
        "np.random.shuffle(shuffle_idx)\n",
        "\n",
        "encoder_input = encoder_input[shuffle_idx]\n",
        "decoder_input = decoder_input[shuffle_idx]\n",
        "decoder_target = decoder_target[shuffle_idx]\n",
        "\n",
        "# 학습과 테스트, 점증 데이터 분리\n",
        "\n",
        "encoder_input_train = encoder_input[:-news_valid_len]\n",
        "decoder_input_train = decoder_input[:-news_valid_len]\n",
        "decoder_target_train = decoder_target[:-news_valid_len]\n",
        "\n",
        "encoder_input_test = encoder_input[-news_valid_len:]\n",
        "decoder_input_test = decoder_input[-news_valid_len:]\n",
        "decoder_target_test = decoder_target[-news_valid_len:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrsVlNliz7Re"
      },
      "source": [
        "전처리를 하면서 `DataFrame.reset_index`를 하였기 때문에 인덱스는 순서대로 정렬되어있으며<br>\n",
        "이 인덱스의 순서를 랜덤하게 섞은 뒤, 각 데이터에 적용시켜 순서를 랜덤하게 섞는다.<br>\n",
        "학습과 검증, 테스트데이터 분리는 2:8비율로 하였다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbFIKeNV_L8U"
      },
      "outputs": [],
      "source": [
        "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
        "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k5YLg8Kz7Re"
      },
      "source": [
        "인코더 문장(`기사`)에 대한 토크나이저를 생성하며<br>\n",
        "단어집합을 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD72yFwFz7Rf"
      },
      "outputs": [],
      "source": [
        "threshold = 6\n",
        "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in src_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print(f\"단어 집합(vocabulary)의 크기 : {total_cnt}\")\n",
        "print(f\"등장 빈도가 {threshold - 1}번 이하인 희귀 단어의 수: {rare_cnt}\")\n",
        "print(f\"단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 {total_cnt - rare_cnt}\")\n",
        "print(f\"단어 집합에서 희귀 단어의 비율: {(rare_cnt / total_cnt)*100}\")\n",
        "print(f\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율: {(rare_freq / total_freq)*100}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFHujVUiz7Rf"
      },
      "source": [
        "전체 빈도 횟수를 6번 미만 빈도의 단어를 선정하였으며<br>\n",
        "빈도 비율은 4%를 넘지 않게 설정하였다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj3Lw_Z8z7Rf"
      },
      "outputs": [],
      "source": [
        "src_vocab = 19448\n",
        "src_tokenizer = Tokenizer(num_words=src_vocab)\n",
        "src_tokenizer.fit_on_texts(encoder_input_train)\n",
        "\n",
        "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
        "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfD5Rj3Lz7Rf"
      },
      "source": [
        "단어 집합을 희귀단어를 제외한 후의 단어집합 크기로 설정하였으며 그 수는 19448개이다.<br>\n",
        "인코더의 택스트에 한해 시퀸스화 시켜준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVObqru9z7Rf"
      },
      "outputs": [],
      "source": [
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql13Cqjsz7Rf"
      },
      "source": [
        "디코더 문장(`요약`)에 대한 토크나이저를 생성하며<br>\n",
        "단어집합을 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r9PRSmhz7Rf"
      },
      "outputs": [],
      "source": [
        "threshold = 3\n",
        "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tar_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print(f\"단어 집합(vocabulary)의 크기 : {total_cnt}\", )\n",
        "print(f\"등장 빈도가 {threshold - 1}번 이하인 희귀 단어의 수: {rare_cnt}\")\n",
        "print(f\"단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 {total_cnt - rare_cnt}\")\n",
        "print(f\"단어 집합에서 희귀 단어의 비율: {(rare_cnt / total_cnt)*100}\")\n",
        "print(f\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율: {(rare_freq / total_freq)*100}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB-9ZRv0z7Rg"
      },
      "source": [
        "전체 빈도 횟수를 3번 미만 빈도의 단어를 선정하였으며<br>\n",
        "빈도 비율은 4%를 넘지 않게 설정하였다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d8hvQI9z7Rg"
      },
      "outputs": [],
      "source": [
        "tar_vocab = 12872\n",
        "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
        "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
        "\n",
        "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
        "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
        "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
        "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd9WA8-iz7Rg"
      },
      "source": [
        "단어 집합을 희귀단어를 제외한 후의 단어집합 크기로 설정하였으며 그 수는 12872개이다.<br>\n",
        "인코더의 택스트에 한해 시퀸스화 시켜준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXJLv7znz7Rg"
      },
      "outputs": [],
      "source": [
        "drop_train = set([index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1])\n",
        "drop_test = set([index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1])\n",
        "\n",
        "print(f\"삭제할 훈련 데이터의 개수 : {len(drop_train)}\")\n",
        "print(f\"삭제할 테스트 데이터의 개수 : {len(drop_test)}\")\n",
        "\n",
        "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
        "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
        "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
        "\n",
        "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
        "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
        "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
        "\n",
        "print(f\"훈련 데이터의 개수 : {len(encoder_input_train)}\")\n",
        "print(f\"훈련 레이블의 개수 : {len(decoder_input_train)}\")\n",
        "print(f\"테스트 데이터의 개수 : {len(encoder_input_test)}\")\n",
        "print(f\"테스트 레이블의 개수 : {len(decoder_input_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_h1AuAFz7Rg"
      },
      "source": [
        "빈 문장을 제거한다.<br>\n",
        "`<sos>`, `<eos>`토큰의 갯수를 포함한 문장이기 때문에 빈 문장의 길이는 1이 된다.<br>\n",
        "실행결과 제거되는 문장은 없다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWWBedAfz7Rg"
      },
      "outputs": [],
      "source": [
        "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
        "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
        "decoder_input_train = pad_sequences(decoder_input_train, maxlen=headlines_max_len, padding='post')\n",
        "decoder_target_train = pad_sequences(decoder_target_train, maxlen=headlines_max_len, padding='post')\n",
        "decoder_input_test = pad_sequences(decoder_input_test, maxlen=headlines_max_len, padding='post')\n",
        "decoder_target_test = pad_sequences(decoder_target_test, maxlen=headlines_max_len, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M4RCBdlz7Rg"
      },
      "source": [
        "각 문장의 길이를 맞추기위해 padding을 뒤에 집어넣는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu9puDTo_MRT"
      },
      "source": [
        "### 모델 설계"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qbNBHBm0LLy"
      },
      "outputs": [],
      "source": [
        "# 인코더\n",
        "encoder_inputs = Input(shape=(text_max_len,), name=\"enc_input\")\n",
        "\n",
        "# 인코더의 임베딩 층\n",
        "enc_emb = Embedding(src_vocab, EMBEDDING_DIM, name=\"enc_emb\")(encoder_inputs)\n",
        "\n",
        "# 인코더의 LSTM 1\n",
        "encoder_lstm1 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4, name=\"enc_lstm_1\")\n",
        "# encoder_lstm1 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True ,dropout = 0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# 인코더의 LSTM 2\n",
        "encoder_lstm2 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4, name=\"enc_lstm_2\")\n",
        "# encoder_lstm2 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True, dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# 인코더의 LSTM 3\n",
        "encoder_lstm3 = LSTM(HIDDEN_SIZE, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4, name=\"enc_lstm_3\")\n",
        "# encoder_lstm3 = LSTM(HIDDEN_SIZE, return_state=True, return_sequences=True, dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4_q_ulpz7Rh"
      },
      "outputs": [],
      "source": [
        "# 디코더 설계\n",
        "decoder_inputs = Input(shape=(None,), name=\"dec_input\")\n",
        "\n",
        "# 디코더의 임베딩 층\n",
        "dec_emb_layer = Embedding(tar_vocab, EMBEDDING_DIM, name=\"dec_emb\")\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 디코더의 LSTM\n",
        "decoder_lstm = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2, name=\"dec_lstm\")\n",
        "# decoder_lstm = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True, dropout=0.4)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn_ITkcoz7Rh"
      },
      "source": [
        "Variational Dropout을 각 lstm레이어마다 구형을 했으며 `recurrent_dropout`은 <br>\n",
        "이전의 정보를 가지는 파라미터기 아닌 새로 들어온 파라미터에 대해서만 dropout을 적용하는 파라미터이다.<br>\n",
        "`NDVIA`에서 제공하는 연산가속 커널 `cuDNN`을 사용하지 못한다는 경고가 나온다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-fXdcdSz7Rh"
      },
      "outputs": [],
      "source": [
        "# 어텐션 층(어텐션 함수)\n",
        "attn_layer = AdditiveAttention(name='attention_layer')\n",
        "\n",
        "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
        "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "\n",
        "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# 디코더의 출력층\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax', name=\"attention_softmax\")\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
        "\n",
        "# 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs, name=\"attention_model\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "t0EMz-pZwBth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3nh-FBOz7Rh"
      },
      "source": [
        "모델의 구조가 복잡하기 때문에 모델도를 그렸으며 레이어는 위와 같다.<br>\n",
        "이전과는 다르게 attention 레이어가 새롭게 생겼으며 38개의 각각의 시퀀스를<br>\n",
        "tanh, softmax등의 계산을 통해 가중치를 구하는 부분이 있음을 알 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy1iw1_slsf3"
      },
      "outputs": [],
      "source": [
        "fit_kwargs['validation_data'] = ([encoder_input_test, decoder_input_test], decoder_target_test)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0002, rho=0.5)\n",
        "    , loss=\"sparse_categorical_crossentropy\", metrics=['acc']\n",
        ")\n",
        "history_dict = model.fit(\n",
        "            x=[encoder_input_train, decoder_input_train]\n",
        "            , y=decoder_target_train\n",
        "            , **fit_kwargs).history\n",
        "\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epoch = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epoch, loss, 'bo', label='Training loss')\n",
        "plt.plot(epoch, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNQ9W93B0LUx"
      },
      "source": [
        "학습결과 우하향하는 그래프의 모습을 보이며 train loss, validation loss 의 차이가 크지않다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv2-aMcEvAs7"
      },
      "source": [
        "### 검증\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1Soi1Am0_wy"
      },
      "outputs": [],
      "source": [
        "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
        "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
        "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tar_word_to_index"
      ],
      "metadata": {
        "id": "zQqHyseJKvtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C7z3IJIz7Ri"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEZRY1YZvA39"
      },
      "outputs": [],
      "source": [
        "# 인코더 설계\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# 이전 시점의 상태들을 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(HIDDEN_SIZE,))\n",
        "decoder_state_input_c = Input(shape=(HIDDEN_SIZE,))\n",
        "\n",
        "dec_emb_inf_enc = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
        "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
        "(\n",
        "    decoder_outputs_inf\n",
        "    , state_h2\n",
        "    , state_c2\n",
        ") = decoder_lstm(dec_emb_inf_enc, initial_state=[decoder_state_input_h, decoder_state_input_c])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_PrcpfDvBDR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6XafcMzz7Ri"
      },
      "outputs": [],
      "source": [
        "# 어텐션 함수\n",
        "decoder_hidden_state_input = Input(shape=(text_max_len, HIDDEN_SIZE))\n",
        "attn_out_inf = attn_layer([decoder_outputs_inf, decoder_hidden_state_input])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs_inf, attn_out_inf])\n",
        "\n",
        "# 디코더의 출력층\n",
        "decoder_outputs_inf = decoder_softmax_layer(decoder_inf_concat)\n",
        "\n",
        "# 최종 디코더 모델\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs_inf] + [state_h2, state_c2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmRUf4jfz7Ri"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPvlKpzY1B5I"
      },
      "outputs": [],
      "source": [
        "data_ori = pd.read_csv(DATA_PATH, encoding='iso-8859-1')[\"text\"]\n",
        "for i in range(50, 100):\n",
        "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
        "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
        "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
        "    print(\"summrize lib 요약 :\", summarize(decode_sequence(data_ori[i]), ratio=0.45))\n",
        "    print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}