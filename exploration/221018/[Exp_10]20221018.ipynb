{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangsae91/project_so/blob/master/exploration/221018/%5BExp_10%5D20221018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "302ZdfCKLlhI",
        "outputId": "c36ec3b2-40b4-4569-9962-5aa12113abb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# google colab전용\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujMb2MtyLrVQ"
      },
      "source": [
        "# exploration 10번째 과제\n",
        "@ 황한용(3기/쏘카)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdrfR8Y3L0ry"
      },
      "source": [
        "## 라이브러리 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LWLP8EFSLqqp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import string # 구두점 정규화 표현식\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`tensorflow.keras.preprocessing.sequence.pad_sequences`는 모듈 위치가 변경되었으므로\n",
        "`tf.keras.utils.pad_sequences`로 변경<br>"
      ],
      "metadata": {
        "id": "dwTie8y2eOO0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qC6hZWKL7x3"
      },
      "source": [
        "## 상수선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XcIjEEgnMD75"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/data/translator_seq2seq\" # 데이터 기본경로\n",
        "DATA_PATH  = BASE_PATH + \"/data/fra.txt\" # 사전 기본\n",
        "\n",
        "SOS_TOKEN = '<sos>' # 문장 시작토큰\n",
        "EOS_TOKEN = '<eos>' # 문장 끝 토큰\n",
        "\n",
        "MAX_SAMPLE_LEN = 88000 # 최대 단어갯수\n",
        "VALID_LEN = 8000 # 검증할 문장의 갯수\n",
        "PUNCTUATION_REGEX = r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]' # 정규화\n",
        "FRENCH_WHITESPACE = r'[\\xa0\\u202f\\u2009]' # whitespace 정규화(1/4 whitespace 등의 대응)\n",
        "FRENCH_APOSTROPHE = r\"’\" # 프랑스어 '\n",
        "FRENCH_DOUBLE_COMMA = r\"'<<|>>'\" # 프랑스어 \"\n",
        "FRENCE_MINUS_REGEX = r\"\\—\" # 프랑스어 -\n",
        "\n",
        "HIDDEN_STATE_NUM = 256 # hidden state의 노드수\n",
        "\n",
        "fit_kwargs = {\n",
        "    \"epochs\":50 # epoch 횟수\n",
        "    , \"batch_size\":512\n",
        "    ,\"validation_data\": None # 추후 추가예정\n",
        "    , \"shuffle\" : True # epoch당 셔플을 할지의 여부\n",
        "    , \"verbose\":1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어의 경우는 `string.punctuation`의 구두점을 활용하여 만들었다.<br>\n",
        "프랑스어의 경우는 문법에 따라 1/8, 1/4, 1/2 `whitespace`를 표준`whitespace`로 변경하였으며<br>\n",
        "`'`, `\"`, `-` 또한 표준으로 변경하였다.<br>\n",
        "알파벳의 악쌍(악센트, 성조)은 단어의 의미를 결정하는 요소이므로 제거하지 않았다."
      ],
      "metadata": {
        "id": "NtHICg5lZmae"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5DV1kd8MPes"
      },
      "source": [
        "## 메인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7XCWajZ0XIj"
      },
      "source": [
        "### 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwkvcfr7MPsm",
        "outputId": "23aaf8d9-6244-43d4-bc7e-e811ac4a5368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플의 수 : 197463\n"
          ]
        }
      ],
      "source": [
        "lines = pd.read_csv(DATA_PATH, names=['eng', 'fra', 'cc'], sep='\\t')\n",
        "print('전체 샘플의 수 :',len(lines))\n",
        "lines.sample(5) #샘플 5개 출력\n",
        "lines.pop('cc')\n",
        "lines = lines.head(MAX_SAMPLE_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXm0KjWe845_"
      },
      "source": [
        "데이터에 대한 설명은 아래와 같다.</br>\n",
        "- eng: 영어문장\n",
        "- fra: 영어 문장에 해당되는 프랑스 문장\n",
        "- cc: 저작권 정보\n",
        "\n",
        "저작권 정보는 데이터 분석에 사용되지 않으므로 로드하지 않았다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "971TB3kw845_",
        "outputId": "01331e89-eead-4a94-fd40-3d4cbcadaea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 단어장의 크기 : 8329\n",
            "프랑스어 단어장의 크기 : 14702\n",
            "영어 시퀀스의 최대 길이 12\n",
            "프랑스어 시퀀스의 최대 길이 19\n"
          ]
        }
      ],
      "source": [
        "# 구두점(Punctuation)을 단어와 분리\n",
        "# 프랑스 문법에만 존재하는 whitespace -> 표준 whitespace로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCH_WHITESPACE, r' ', regex=True)\n",
        "#프랑스 문법에만 존재하는 ’를 '로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCH_APOSTROPHE, r\"'\", regex=True)\n",
        "# 프랑스 문법에만 존재하는 <<, >>를 \"로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCH_DOUBLE_COMMA, r'\"', regex=True)\n",
        "# 프랑스 문법에만 존재하는 —를 -로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCE_MINUS_REGEX, r\"-\", regex=True)\n",
        "\n",
        "# 구분점에 whitespace를 지음\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(PUNCTUATION_REGEX, r' \\g<0> ', regex=True).replace(' +', ' ', regex=True).str.strip()\n",
        "lines[\"eng\"] = lines[\"eng\"].str.replace(PUNCTUATION_REGEX, r' \\g<0> ', regex=True).replace(' +', ' ', regex=True).str.strip()\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(' +', ' ', regex=True)\n",
        "lines[\"eng\"] = lines[\"eng\"].str.replace(' +', ' ', regex=True)\n",
        "\n",
        "lines[\"fra_decoder_input\"] = f'{SOS_TOKEN} '+ lines[\"fra\"]\n",
        "lines[\"fra_decoder_target\"] = lines[\"fra\"] + f' {EOS_TOKEN}'\n",
        "lines[\"fra\"] = f'{SOS_TOKEN} '+ lines[\"fra\"] + f' {EOS_TOKEN}' # 양옆에 문장의 시작과 끝의 테그를 붙인다.\n",
        "\n",
        "lines[\"eng\"] = lines[\"eng\"].str.split()\n",
        "\n",
        "lines[\"fra\"] = lines[\"fra\"].str.split()\n",
        "lines[\"fra_decoder_input\"] = lines[\"fra_decoder_input\"].str.split()\n",
        "lines[\"fra_decoder_target\"] = lines[\"fra_decoder_target\"].str.split()\n",
        "\n",
        "eng_tokenizer = Tokenizer(filters=\"\")  # 문자 단위로 Tokenizer를 생성 \n",
        "eng_tokenizer.fit_on_texts(lines[\"eng\"])\n",
        "encoder_input = eng_tokenizer.texts_to_sequences(lines[\"eng\"])\n",
        "\n",
        "fra_tokenizer = Tokenizer(filters=\"\")\n",
        "fra_tokenizer.fit_on_texts(lines[\"fra\"])\n",
        "\n",
        "decoder_input = fra_tokenizer.texts_to_sequences(lines[\"fra_decoder_input\"])\n",
        "decoder_target = fra_tokenizer.texts_to_sequences(lines[\"fra_decoder_target\"])\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
        "print('영어 단어장의 크기 :', eng_vocab_size)\n",
        "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
        "\n",
        "max_eng_seq_len = max(map(len, encoder_input))\n",
        "max_fra_seq_len = max(map(len, decoder_input))\n",
        "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
        "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어문장와 프랑스문장를 각각 tokenize한 뒤<br>\n",
        "단어 사전의크기, 한 문장의 최대 단어 갯수를 확인<br>\n",
        "이 데이터는 후에 padding에 사용할 예정이다.<br>\n",
        "문장의 끝 토큰이 제거된 input과<br>\n",
        "문장의 시작 토큰이 제거된 output을 각각 데이터 프레임에서 생산하였으며<br>\n",
        "이번에는 구두점도 임배딩에 필요하므로 필터옵션을 통해 없어지지 않도록 설정하였다.<br>"
      ],
      "metadata": {
        "id": "ajtkCZfqI5ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
        "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
        "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
        "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
        "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX-uCKCAK1FQ",
        "outputId": "36e2c897-e344-473f-e324-c563b82f8b8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 데이터의 크기(shape) : (88000, 12)\n",
            "프랑스어 입력데이터의 크기(shape) : (88000, 19)\n",
            "프랑스어 출력데이터의 크기(shape) : (88000, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 언어의 최대 단어갯수에 맞춰 padding을 생성하였다.<br>\n",
        "(`문장의 갯수`,`각 언어의 문장당 단어 최대갯수`)모양의 데이터가 생성되었다.\n"
      ],
      "metadata": {
        "id": "D9kNM2TiK1RF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HbFIKeNV_L8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cfe265c-573c-4617-cfd4-0d71af916d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 16 110   2   9 875   4   1   0   0   0   0   0]\n",
            "[    1    21    14     9 11847    10     3     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0]\n",
            "[   21    14     9 11847    10     3     2     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0]\n"
          ]
        }
      ],
      "source": [
        "shuffle_idx  = np.arange(MAX_SAMPLE_LEN)\n",
        "np.random.shuffle(shuffle_idx)\n",
        "\n",
        "encoder_input = encoder_input[shuffle_idx]\n",
        "decoder_input = decoder_input[shuffle_idx]\n",
        "decoder_target = decoder_target[shuffle_idx]\n",
        "\n",
        "print(encoder_input[3])\n",
        "print(decoder_input[3])\n",
        "print(decoder_target[3])\n",
        "\n",
        "encoder_input_train = encoder_input[:-VALID_LEN]\n",
        "decoder_input_train = decoder_input[:-VALID_LEN]\n",
        "decoder_target_train = decoder_target[:-VALID_LEN]\n",
        "\n",
        "encoder_input_test = encoder_input[-VALID_LEN:]\n",
        "decoder_input_test = decoder_input[-VALID_LEN:]\n",
        "decoder_target_test = decoder_target[-VALID_LEN:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQlTE3xms-k0"
      },
      "source": [
        "과적합 방지를 위해 학습과 점증을 10:1비율로 나누었다.<br>\n",
        "문장셈플은 인덱스를 렌덤값을 통해 섞어서 순서를 바꿨으며<br>\n",
        "임의로 하나의 값을 뽑았을 시<br>\n",
        "`decoder_input`과 `decoder_target`의 값은 `<sos>`, `<eos>`의 차이이므로 올바르게 섞인것을 확인할 수 있다.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu9puDTo_MRT"
      },
      "source": [
        "### 모델 설계"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 과학습을 하지 않는 모델(이론)"
      ],
      "metadata": {
        "id": "XDQWHvL74DO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2qbNBHBm0LLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3baa45d-fe44-4d4c-9f31-6d1f207fe7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"seq2seq_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 256)    2132224     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    3763712     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " masking (Masking)              (None, None, 256)    0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " masking_1 (Masking)            (None, None, 256)    0           ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        525312      ['masking[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  525312      ['masking_1[0][0]',              \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 14702)  3778414     ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,724,974\n",
            "Trainable params: 10,724,974\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embadding = Embedding(eng_vocab_size, HIDDEN_STATE_NUM)\n",
        "encoder_masking = Masking(mask_value=0.0)(encoder_embadding(encoder_inputs))\n",
        "encoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_state = True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_masking)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "decoder_embadding = Embedding(fra_vocab_size, HIDDEN_STATE_NUM)\n",
        "decoder_masking = Masking(mask_value=0.0)(decoder_embadding(decoder_inputs))\n",
        "decoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_sequences = True, return_state=True)\n",
        "decoder_outputs, _, _= decoder_lstm(decoder_masking, initial_state = encoder_states)\n",
        "\n",
        "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"seq2seq_model\")\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "    , loss=\"sparse_categorical_crossentropy\", metrics=['acc']\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "encorder는 영어를 프랑스어로 번역하기 위한 레이어이므로<br>\n",
        "decorder의 input에 맞게 모델을 구성하였으며<br>\n",
        "one-hot 인코딩을 하지 않은 상태이므로 `sparse_categorical_crossentropy`로 설정하였다."
      ],
      "metadata": {
        "id": "yDMdVE5Grafh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 과학습을 하는 모델(실제)"
      ],
      "metadata": {
        "id": "W-F9UWIh4bbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overfit_encoder_inputs = Input(shape=(None,))\n",
        "overfit_encoder_embadding = Embedding(eng_vocab_size, HIDDEN_STATE_NUM)\n",
        "overfit_encoder_masking = Masking(mask_value=0.0)(overfit_encoder_embadding(overfit_encoder_inputs))\n",
        "overfit_encoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_state = True)\n",
        "overfit_encoder_outputs, overfit_state_h, overfit_state_c = overfit_encoder_lstm(overfit_encoder_masking)\n",
        "overfit_encoder_states = [overfit_state_h, overfit_state_c]\n",
        "\n",
        "overfit_decoder_inputs = Input(shape=(None, ))\n",
        "overfit_decoder_embadding = Embedding(fra_vocab_size, HIDDEN_STATE_NUM)\n",
        "overfit_decoder_masking = Masking(mask_value=0.0)(overfit_decoder_embadding(overfit_decoder_inputs))\n",
        "overfit_decoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_sequences = True, return_state=True)\n",
        "overfit_decoder_outputs, _, _= overfit_decoder_lstm(overfit_decoder_masking, initial_state = overfit_encoder_states)\n",
        "\n",
        "overfit_decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
        "overfit_decoder_outputs = overfit_decoder_softmax_layer(overfit_decoder_outputs)\n",
        "\n",
        "overfit_model = Model([overfit_encoder_inputs, overfit_decoder_inputs], overfit_decoder_outputs, name=\"overfit_seq2seq_model\")\n",
        "overfit_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.005)\n",
        "    , loss=\"sparse_categorical_crossentropy\", metrics=['acc']\n",
        ")\n",
        "overfit_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op3kYHQE4Xcu",
        "outputId": "23ec9004-0fa7-4d84-abde-c38f3fc6a8ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"overfit_seq2seq_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, None, 256)    2132224     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 256)    3763712     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " masking_2 (Masking)            (None, None, 256)    0           ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " masking_3 (Masking)            (None, None, 256)    0           ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 256),        525312      ['masking_2[0][0]']              \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['masking_3[0][0]',              \n",
            "                                 (None, 256),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 14702)  3778414     ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,724,974\n",
            "Trainable params: 10,724,974\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전의 과학습 아닌 모델과 다른 점은 optimizer부분에 running rate의 차이점밖에 없다."
      ],
      "metadata": {
        "id": "nRrqhFjb5sZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습"
      ],
      "metadata": {
        "id": "ajgB-k1RFH1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit_kwargs[\"validation_data\"] = ([encoder_input_test, decoder_input_test], decoder_target_test)\n",
        "\n",
        "history_dict = model.fit(\n",
        "                         x=[encoder_input_train, decoder_input_train]\n",
        "                         , y=decoder_target_train\n",
        "                         , **fit_kwargs).history\n",
        "\n",
        "overfit_history_dict = overfit_model.fit(\n",
        "                            x=[encoder_input_train, decoder_input_train]\n",
        "                            , y=decoder_target_train\n",
        "                            , **fit_kwargs).history\n",
        "\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "overfit_loss = overfit_history_dict['loss']\n",
        "overfit_val_loss = overfit_history_dict['val_loss']\n",
        "\n",
        "epoch = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epoch, loss, 'bo', label='Training loss')\n",
        "plt.plot(epoch, val_loss, 'b', label='Validation loss')\n",
        "plt.plot(epoch, loss, 'ro', label='Overfit training loss')\n",
        "plt.plot(epoch, val_loss, 'r', label='Overfit validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hy1iw1_slsf3",
        "outputId": "d7f9e03d-e458-45d8-f9e1-f439a75f3784"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "157/157 [==============================] - 33s 158ms/step - loss: 1.9458 - acc: 0.7196 - val_loss: 1.3087 - val_acc: 0.7820\n",
            "Epoch 2/50\n",
            "157/157 [==============================] - 22s 142ms/step - loss: 1.1054 - acc: 0.8060 - val_loss: 0.9979 - val_acc: 0.8213\n",
            "Epoch 3/50\n",
            "157/157 [==============================] - 23s 144ms/step - loss: 0.8674 - acc: 0.8336 - val_loss: 0.8675 - val_acc: 0.8395\n",
            "Epoch 4/50\n",
            "157/157 [==============================] - 23s 146ms/step - loss: 0.7192 - acc: 0.8519 - val_loss: 0.7876 - val_acc: 0.8506\n",
            "Epoch 5/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.6099 - acc: 0.8662 - val_loss: 0.7310 - val_acc: 0.8591\n",
            "Epoch 6/50\n",
            "157/157 [==============================] - 23s 146ms/step - loss: 0.5260 - acc: 0.8779 - val_loss: 0.6953 - val_acc: 0.8655\n",
            "Epoch 7/50\n",
            "157/157 [==============================] - 23s 146ms/step - loss: 0.4589 - acc: 0.8887 - val_loss: 0.6688 - val_acc: 0.8706\n",
            "Epoch 8/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.4047 - acc: 0.8980 - val_loss: 0.6533 - val_acc: 0.8735\n",
            "Epoch 9/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.3620 - acc: 0.9057 - val_loss: 0.6432 - val_acc: 0.8760\n",
            "Epoch 10/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.3273 - acc: 0.9123 - val_loss: 0.6339 - val_acc: 0.8787\n",
            "Epoch 11/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.2981 - acc: 0.9187 - val_loss: 0.6355 - val_acc: 0.8799\n",
            "Epoch 12/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.2764 - acc: 0.9232 - val_loss: 0.6340 - val_acc: 0.8813\n",
            "Epoch 13/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.2567 - acc: 0.9275 - val_loss: 0.6331 - val_acc: 0.8829\n",
            "Epoch 14/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.2414 - acc: 0.9310 - val_loss: 0.6387 - val_acc: 0.8835\n",
            "Epoch 15/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.2284 - acc: 0.9340 - val_loss: 0.6432 - val_acc: 0.8844\n",
            "Epoch 16/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.2182 - acc: 0.9363 - val_loss: 0.6463 - val_acc: 0.8838\n",
            "Epoch 17/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.2094 - acc: 0.9381 - val_loss: 0.6517 - val_acc: 0.8850\n",
            "Epoch 18/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.2031 - acc: 0.9393 - val_loss: 0.6592 - val_acc: 0.8850\n",
            "Epoch 19/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1969 - acc: 0.9408 - val_loss: 0.6608 - val_acc: 0.8852\n",
            "Epoch 20/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1907 - acc: 0.9423 - val_loss: 0.6670 - val_acc: 0.8851\n",
            "Epoch 21/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1860 - acc: 0.9433 - val_loss: 0.6750 - val_acc: 0.8852\n",
            "Epoch 22/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1833 - acc: 0.9438 - val_loss: 0.6788 - val_acc: 0.8853\n",
            "Epoch 23/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1799 - acc: 0.9444 - val_loss: 0.6848 - val_acc: 0.8853\n",
            "Epoch 24/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1752 - acc: 0.9456 - val_loss: 0.6921 - val_acc: 0.8853\n",
            "Epoch 25/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1725 - acc: 0.9462 - val_loss: 0.6980 - val_acc: 0.8852\n",
            "Epoch 26/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1707 - acc: 0.9465 - val_loss: 0.7025 - val_acc: 0.8850\n",
            "Epoch 27/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1681 - acc: 0.9473 - val_loss: 0.7090 - val_acc: 0.8847\n",
            "Epoch 28/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1666 - acc: 0.9474 - val_loss: 0.7149 - val_acc: 0.8848\n",
            "Epoch 29/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1660 - acc: 0.9474 - val_loss: 0.7214 - val_acc: 0.8838\n",
            "Epoch 30/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1650 - acc: 0.9475 - val_loss: 0.7257 - val_acc: 0.8840\n",
            "Epoch 31/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1639 - acc: 0.9478 - val_loss: 0.7323 - val_acc: 0.8835\n",
            "Epoch 32/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1633 - acc: 0.9480 - val_loss: 0.7395 - val_acc: 0.8842\n",
            "Epoch 33/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1613 - acc: 0.9484 - val_loss: 0.7442 - val_acc: 0.8844\n",
            "Epoch 34/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1619 - acc: 0.9483 - val_loss: 0.7515 - val_acc: 0.8843\n",
            "Epoch 35/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1606 - acc: 0.9483 - val_loss: 0.7552 - val_acc: 0.8846\n",
            "Epoch 36/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1597 - acc: 0.9487 - val_loss: 0.7611 - val_acc: 0.8843\n",
            "Epoch 37/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1591 - acc: 0.9488 - val_loss: 0.7630 - val_acc: 0.8846\n",
            "Epoch 38/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1568 - acc: 0.9493 - val_loss: 0.7672 - val_acc: 0.8837\n",
            "Epoch 39/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1568 - acc: 0.9494 - val_loss: 0.7719 - val_acc: 0.8838\n",
            "Epoch 40/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1553 - acc: 0.9497 - val_loss: 0.7787 - val_acc: 0.8830\n",
            "Epoch 41/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1553 - acc: 0.9497 - val_loss: 0.7831 - val_acc: 0.8837\n",
            "Epoch 42/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1565 - acc: 0.9494 - val_loss: 0.7875 - val_acc: 0.8839\n",
            "Epoch 43/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1546 - acc: 0.9497 - val_loss: 0.7863 - val_acc: 0.8831\n",
            "Epoch 44/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1549 - acc: 0.9495 - val_loss: 0.7938 - val_acc: 0.8835\n",
            "Epoch 45/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1528 - acc: 0.9501 - val_loss: 0.8005 - val_acc: 0.8831\n",
            "Epoch 46/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1535 - acc: 0.9501 - val_loss: 0.8017 - val_acc: 0.8837\n",
            "Epoch 47/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1548 - acc: 0.9496 - val_loss: 0.8085 - val_acc: 0.8823\n",
            "Epoch 48/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1536 - acc: 0.9498 - val_loss: 0.8110 - val_acc: 0.8819\n",
            "Epoch 49/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1528 - acc: 0.9501 - val_loss: 0.8085 - val_acc: 0.8826\n",
            "Epoch 50/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1535 - acc: 0.9497 - val_loss: 0.8150 - val_acc: 0.8826\n",
            "Epoch 1/50\n",
            "157/157 [==============================] - 32s 160ms/step - loss: 2.3350 - acc: 0.6606 - val_loss: 1.8364 - val_acc: 0.7295\n",
            "Epoch 2/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.5833 - acc: 0.7612 - val_loss: 1.4114 - val_acc: 0.7848\n",
            "Epoch 3/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.2218 - acc: 0.8018 - val_loss: 1.0924 - val_acc: 0.8151\n",
            "Epoch 4/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.9398 - acc: 0.8305 - val_loss: 0.8857 - val_acc: 0.8395\n",
            "Epoch 5/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.7397 - acc: 0.8550 - val_loss: 0.7518 - val_acc: 0.8580\n",
            "Epoch 6/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.5909 - acc: 0.8758 - val_loss: 0.6610 - val_acc: 0.8716\n",
            "Epoch 7/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.4760 - acc: 0.8929 - val_loss: 0.5967 - val_acc: 0.8817\n",
            "Epoch 8/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.3880 - acc: 0.9076 - val_loss: 0.5578 - val_acc: 0.8890\n",
            "Epoch 9/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.3226 - acc: 0.9193 - val_loss: 0.5312 - val_acc: 0.8932\n",
            "Epoch 10/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.2736 - acc: 0.9289 - val_loss: 0.5172 - val_acc: 0.8961\n",
            "Epoch 11/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.2365 - acc: 0.9365 - val_loss: 0.5094 - val_acc: 0.8986\n",
            "Epoch 12/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.2082 - acc: 0.9425 - val_loss: 0.5072 - val_acc: 0.8990\n",
            "Epoch 13/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1855 - acc: 0.9475 - val_loss: 0.5032 - val_acc: 0.9008\n",
            "Epoch 14/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1676 - acc: 0.9512 - val_loss: 0.5079 - val_acc: 0.9012\n",
            "Epoch 15/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1534 - acc: 0.9543 - val_loss: 0.5101 - val_acc: 0.9016\n",
            "Epoch 16/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1415 - acc: 0.9571 - val_loss: 0.5159 - val_acc: 0.9015\n",
            "Epoch 17/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1315 - acc: 0.9593 - val_loss: 0.5197 - val_acc: 0.9014\n",
            "Epoch 18/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1235 - acc: 0.9610 - val_loss: 0.5245 - val_acc: 0.9017\n",
            "Epoch 19/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1164 - acc: 0.9627 - val_loss: 0.5291 - val_acc: 0.9027\n",
            "Epoch 20/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1103 - acc: 0.9641 - val_loss: 0.5350 - val_acc: 0.9017\n",
            "Epoch 21/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1053 - acc: 0.9652 - val_loss: 0.5397 - val_acc: 0.9025\n",
            "Epoch 22/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.1007 - acc: 0.9663 - val_loss: 0.5451 - val_acc: 0.9025\n",
            "Epoch 23/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0969 - acc: 0.9670 - val_loss: 0.5526 - val_acc: 0.9013\n",
            "Epoch 24/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0945 - acc: 0.9675 - val_loss: 0.5584 - val_acc: 0.9015\n",
            "Epoch 25/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0914 - acc: 0.9683 - val_loss: 0.5653 - val_acc: 0.9021\n",
            "Epoch 26/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0892 - acc: 0.9689 - val_loss: 0.5695 - val_acc: 0.9010\n",
            "Epoch 27/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0872 - acc: 0.9693 - val_loss: 0.5732 - val_acc: 0.9011\n",
            "Epoch 28/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0851 - acc: 0.9696 - val_loss: 0.5796 - val_acc: 0.9010\n",
            "Epoch 29/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0835 - acc: 0.9701 - val_loss: 0.5853 - val_acc: 0.9012\n",
            "Epoch 30/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0823 - acc: 0.9704 - val_loss: 0.5883 - val_acc: 0.9005\n",
            "Epoch 31/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0813 - acc: 0.9705 - val_loss: 0.5968 - val_acc: 0.9003\n",
            "Epoch 32/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0803 - acc: 0.9707 - val_loss: 0.6000 - val_acc: 0.9010\n",
            "Epoch 33/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0792 - acc: 0.9708 - val_loss: 0.6053 - val_acc: 0.9002\n",
            "Epoch 34/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0781 - acc: 0.9712 - val_loss: 0.6075 - val_acc: 0.9005\n",
            "Epoch 35/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0772 - acc: 0.9713 - val_loss: 0.6145 - val_acc: 0.9005\n",
            "Epoch 36/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0763 - acc: 0.9715 - val_loss: 0.6186 - val_acc: 0.9001\n",
            "Epoch 37/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0759 - acc: 0.9715 - val_loss: 0.6215 - val_acc: 0.9001\n",
            "Epoch 38/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0755 - acc: 0.9717 - val_loss: 0.6254 - val_acc: 0.9002\n",
            "Epoch 39/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0755 - acc: 0.9715 - val_loss: 0.6304 - val_acc: 0.9001\n",
            "Epoch 40/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0747 - acc: 0.9718 - val_loss: 0.6337 - val_acc: 0.9000\n",
            "Epoch 41/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0741 - acc: 0.9720 - val_loss: 0.6403 - val_acc: 0.8997\n",
            "Epoch 42/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0735 - acc: 0.9720 - val_loss: 0.6407 - val_acc: 0.8995\n",
            "Epoch 43/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0734 - acc: 0.9720 - val_loss: 0.6454 - val_acc: 0.9001\n",
            "Epoch 44/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0725 - acc: 0.9723 - val_loss: 0.6508 - val_acc: 0.8994\n",
            "Epoch 45/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0718 - acc: 0.9724 - val_loss: 0.6499 - val_acc: 0.8999\n",
            "Epoch 46/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0710 - acc: 0.9726 - val_loss: 0.6515 - val_acc: 0.8988\n",
            "Epoch 47/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0707 - acc: 0.9726 - val_loss: 0.6577 - val_acc: 0.8995\n",
            "Epoch 48/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0705 - acc: 0.9726 - val_loss: 0.6604 - val_acc: 0.8988\n",
            "Epoch 49/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0703 - acc: 0.9727 - val_loss: 0.6652 - val_acc: 0.8991\n",
            "Epoch 50/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0712 - acc: 0.9724 - val_loss: 0.6698 - val_acc: 0.8984\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNQ9W93B0LUx"
      },
      "source": [
        "입력값은 똑같이 적용하였으며<br>\n",
        "학습결과\n",
        "- overfit이 되지않는 모델의 경우는 정상적으로 우하향하는 그래프의 모습을 보이며<br>train loss, validation loss 의 차이가 크지않다.\n",
        "- overfit이 된 모델은 대략 epoch이 8 이후부터 이상점이 드러나며<br>\n",
        "어느 기점으로 validation loss의 값이 증가하는 모습을 볼 수 있다.\n",
        "\n",
        "라는 결과를 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 검증\n"
      ],
      "metadata": {
        "id": "mv2-aMcEvAs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################<<<nomal>>>##################################################################\n",
        "\n",
        "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
        "encoder_model.summary()\n",
        "\n",
        "# 이전 time step의 hidden state를 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 cell state를 저장하는 텐서\n",
        "decoder_state_input_c = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_embadding_val = decoder_embadding(decoder_inputs)\n",
        "\n",
        "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
        "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
        "decoder_outputs_val, state_h, state_c = decoder_lstm(decoder_embadding_val, initial_state = decoder_states_inputs)\n",
        "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
        "decoder_states_val = [state_h, state_c]\n",
        "\n",
        "decoder_outputs_val = decoder_softmax_layer(decoder_outputs_val)\n",
        "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs_val] + decoder_states_val, name=\"teaching_model\")\n",
        "decoder_model.summary()\n",
        "\n",
        "###################################################<<<overfit>>>##################################################################\n",
        "\n",
        "overfit_encoder_model = Model(inputs = overfit_encoder_inputs, outputs = overfit_encoder_states)\n",
        "overfit_encoder_model.summary()\n",
        "\n",
        "# 이전 time step의 hidden state를 저장하는 텐서\n",
        "overfit_decoder_state_input_h = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 cell state를 저장하는 텐서\n",
        "overfit_decoder_state_input_c = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
        "overfit_decoder_states_inputs = [overfit_decoder_state_input_h, overfit_decoder_state_input_c]\n",
        "\n",
        "overfit_decoder_embadding_val = overfit_decoder_embadding(overfit_decoder_inputs)\n",
        "\n",
        "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
        "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
        "overfit_decoder_outputs_val, overfit_state_h, overfit_state_c = overfit_decoder_lstm(overfit_decoder_embadding_val, initial_state = overfit_decoder_states_inputs)\n",
        "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
        "overfit_decoder_states_val = [overfit_state_h, overfit_state_c]\n",
        "\n",
        "overfit_decoder_outputs_val = overfit_decoder_softmax_layer(overfit_decoder_outputs_val)\n",
        "overfit_decoder_model = Model(inputs=[overfit_decoder_inputs] + overfit_decoder_states_inputs, outputs=[overfit_decoder_outputs_val] + overfit_decoder_states_val, name=\"overfit_teaching_model\")\n",
        "overfit_decoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEZRY1YZvA39",
        "outputId": "0c7187e7-aba4-4fac-975a-47fc2ec6931f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         2132224   \n",
            "                                                                 \n",
            " masking (Masking)           (None, None, 256)         0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 [(None, 256),             525312    \n",
            "                              (None, 256),                       \n",
            "                              (None, 256)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,657,536\n",
            "Trainable params: 2,657,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"teaching_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    3763712     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  525312      ['embedding_1[1][0]',            \n",
            "                                 (None, 256),                     'input_5[0][0]',                \n",
            "                                 (None, 256)]                     'input_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 14702)  3778414     ['lstm_1[1][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,067,438\n",
            "Trainable params: 8,067,438\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         2132224   \n",
            "                                                                 \n",
            " masking_2 (Masking)         (None, None, 256)         0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               [(None, 256),             525312    \n",
            "                              (None, 256),                       \n",
            "                              (None, 256)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,657,536\n",
            "Trainable params: 2,657,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"overfit_teaching_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 256)    3763712     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " input_7 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['embedding_3[1][0]',            \n",
            "                                 (None, 256),                     'input_7[0][0]',                \n",
            "                                 (None, 256)]                     'input_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 14702)  3778414     ['lstm_3[1][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,067,438\n",
            "Trainable params: 8,067,438\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습이 과적합된 모델 혹은 아닌모델 두 경우의 teaching forcing을 구현하였다."
      ],
      "metadata": {
        "id": "6_PrcpfDvBDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng2idx = eng_tokenizer.word_index\n",
        "fra2idx = fra_tokenizer.word_index\n",
        "idx2eng = eng_tokenizer.index_word\n",
        "idx2fra = fra_tokenizer.index_word"
      ],
      "metadata": {
        "id": "m1Soi1Am0_wy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 -> 인덱스, 인덱스 -> 단어에 해당하는 데이터 생성"
      ],
      "metadata": {
        "id": "9Q_oqPgi0_5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = fra2idx[SOS_TOKEN]\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    # stop_condition이 True가 될 때까지 루프 반복\n",
        "    while not stop_condition:\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # 예측 결과를 문자로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = idx2fra[sampled_token_index]\n",
        "\n",
        "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
        "        decoded_sentence += \" \" + sampled_char\n",
        "\n",
        "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "        if (sampled_char == EOS_TOKEN or\n",
        "           len(decoded_sentence) > fit_kwargs[\"epochs\"]):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "wPvlKpzY1B5I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "예제에 있던 코드를 가져왔으며 ont-hot vector에 해당하는 부분을 입/출력값에 맞게 조정하였다.<br>\n",
        "이번에는 과적합 여부에 관련된 모델의 출력값을 비교가 필요하므로<br>\n",
        "각 모델의 조건에 따라 결과값을 출력하는 함수로 변경하였다.<br>"
      ],
      "metadata": {
        "id": "EXgwX_Ff1CEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 15\n",
        "input_seq = encoder_input_train[sample: sample + 1]\n",
        "\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"입력문장 :\", \"\".join([\n",
        "        idx2eng[encoded_word] + \" \"\n",
        "        for encoded_word in encoder_input_train[sample]\n",
        "        if encoded_word != 0\n",
        "    ])\n",
        ")\n",
        "print(\"정답문장 :\",  \"\".join([\n",
        "      idx2fra[encoded_word] + \" \"\n",
        "      for encoded_word in decoder_input_train[sample]\n",
        "      if encoded_word != 0 and encoded_word != fra2idx[SOS_TOKEN] and encoded_word != fra2idx[EOS_TOKEN]\n",
        "  ])\n",
        ")\n",
        "\n",
        "print(\"과적합이 안된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, encoder_model, decoder_model)[1:-5])\n",
        "print(\"과적합된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, overfit_encoder_model, overfit_decoder_model)[1:-5])\n",
        "\n",
        "print(\"-\" * 35)\n",
        "\n",
        "input_seq = encoder_input_test[sample: sample + 1]\n",
        "\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"입력문장 :\", \"\".join([\n",
        "        idx2eng[encoded_word] + \" \"\n",
        "        for encoded_word in encoder_input_test[sample]\n",
        "        if encoded_word != 0\n",
        "    ])\n",
        ")\n",
        "print(\"정답문장 :\",  \"\".join([\n",
        "      idx2fra[encoded_word] + \" \"\n",
        "      for encoded_word in decoder_input_test[sample]\n",
        "      if encoded_word != 0 and encoded_word != fra2idx[SOS_TOKEN] and encoded_word != fra2idx[EOS_TOKEN]\n",
        "  ])\n",
        ")\n",
        "\n",
        "print(\"과적합이 안된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, encoder_model, decoder_model)[1:-5])\n",
        "print(\"과적합된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, overfit_encoder_model, overfit_decoder_model)[1:-5])\n",
        "\n",
        "print(\"-\" * 35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5eU9KvT1XZl",
        "outputId": "3cde0217-2eff-420e-cb8b-c69d0779a323"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "입력문장 : i ' ll make you some coffee . \n",
            "정답문장 : je vous ferai du café . \n",
            "과적합이 안된 모델\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 328ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "번역문장 : je vous ferai du café . \n",
            "과적합된 모델\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 656ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "번역문장 : je vous ferai du café . \n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "입력문장 : why is life so difficult ? \n",
            "정답문장 : pourquoi la vie est - elle si difficile ? \n",
            "과적합이 안된 모델\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "번역문장 : pourquoi est - ce que ça te manque ? \n",
            "과적합된 모델\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "번역문장 : pourquoi est - ce que la vie difficile de moi ? \n",
            "-----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E7-ju2Gp1Xko"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsx8OVYGnM-s"
      },
      "source": [
        "### 회고\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ngdabGo3j4l"
      },
      "source": [
        "- 학습된 데이터가 전체적으로 높은 정확도를 보여 미리 학습된 데이터의 중요성을 알게되었다.\n",
        "- 항상 학습된 데이터가 높은 정확도를 나타내는 것이 아니라 하이퍼파라미터의 튜닝에 따라 결과가 달라질 수 있다는 것을 알게 되었다.\n",
        "\n",
        "※ 이번 레포트는 양희성님의 모델구조의 조언으로 작성되었음을 알려드립니다.<br>\n",
        "   희성님에게 감사하다는 글을 남기며 이만 글을 마치겠습니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}