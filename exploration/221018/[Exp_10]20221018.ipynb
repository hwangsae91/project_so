{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangsae91/project_so/blob/master/exploration/221018/%5BExp_10%5D20221018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "302ZdfCKLlhI",
        "outputId": "7a3ab503-0c97-4b84-ada7-ef565f8f3466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# google colab전용\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujMb2MtyLrVQ"
      },
      "source": [
        "# exploration 10번째 과제\n",
        "@ 황한용(3기/쏘카)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdrfR8Y3L0ry"
      },
      "source": [
        "## 라이브러리 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LWLP8EFSLqqp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import string # 구두점 정규화 표현식\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`tensorflow.keras.preprocessing.sequence.pad_sequences`는 모듈 위치가 변경되었으므로\n",
        "`tf.keras.utils.pad_sequences`로 변경<br>"
      ],
      "metadata": {
        "id": "dwTie8y2eOO0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qC6hZWKL7x3"
      },
      "source": [
        "## 상수선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XcIjEEgnMD75"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/data/translator_seq2seq\" # 데이터 기본경로\n",
        "DATA_PATH  = BASE_PATH + \"/data/fra.txt\" # 사전 기본\n",
        "\n",
        "SOS_TOKEN = '<sos>' # 문장 시작토큰\n",
        "EOS_TOKEN = '<eos>' # 문장 끝 토큰\n",
        "\n",
        "MAX_SAMPLE_LEN = 55000 # 최대 단어갯수\n",
        "VALID_LEN = 5000 # 검증할 문장의 갯수\n",
        "PUNCTUATION_REGEX = r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]' # 정규화\n",
        "FRENCH_WHITESPACE = r'[\\xa0\\u202f\\u2009]' # whitespace 정규화(1/4 whitespace 등의 대응)\n",
        "FRENCH_APOSTROPHE = r\"’\" # 프랑스어 '\n",
        "FRENCH_DOUBLE_COMMA = r\"'<<|>>'\" # 프랑스어 \"\n",
        "FRENCE_MINUS_REGEX = r\"\\—\" # 프랑스어 -\n",
        "\n",
        "HIDDEN_STATE_NUM = 64 # hidden state의 노드수\n",
        "\n",
        "fit_kwargs = {\n",
        "    \"epochs\":50 # epoch 횟수\n",
        "    , \"batch_size\":HIDDEN_STATE_NUM\n",
        "    ,\"validation_data\": None # 추후 추가예정\n",
        "    , \"shuffle\" : True # epoch당 셔플을 할지의 여부\n",
        "    , \"verbose\":1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어의 경우는 `string.punctuation`의 구두점을 활용하여 만들었다.<br>\n",
        "프랑스어의 경우는 문법에 따라 1/8, 1/4, 1/2 `whitespace`를 표준`whitespace`로 변경하였으며<br>\n",
        "`'`, `\"`, `-` 또한 표준으로 변경하였다.<br>\n",
        "알파벳의 악쌍(악센트, 성조)은 단어의 의미를 결정하는 요소이므로 제거하지 않았다."
      ],
      "metadata": {
        "id": "NtHICg5lZmae"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5DV1kd8MPes"
      },
      "source": [
        "## 메인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7XCWajZ0XIj"
      },
      "source": [
        "### 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwkvcfr7MPsm",
        "outputId": "3d2bde08-a1e3-40eb-d529-2ac189ede9fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플의 수 : 197463\n"
          ]
        }
      ],
      "source": [
        "lines = pd.read_csv(DATA_PATH, names=['eng', 'fra', 'cc'], sep='\\t')\n",
        "print('전체 샘플의 수 :',len(lines))\n",
        "lines.sample(5) #샘플 5개 출력\n",
        "lines.pop('cc')\n",
        "lines = lines.head(MAX_SAMPLE_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXm0KjWe845_"
      },
      "source": [
        "데이터에 대한 설명은 아래와 같다.</br>\n",
        "- eng: 영어문장\n",
        "- fra: 영어 문장에 해당되는 프랑스 문장\n",
        "- cc: 저작권 정보\n",
        "\n",
        "저작권 정보는 데이터 분석에 사용되지 않으므로 로드하지 않았다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "971TB3kw845_",
        "outputId": "e3afab67-7b6e-4edd-f14c-3fcf93235677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 단어장의 크기 : 6386\n",
            "프랑스어 단어장의 크기 : 11450\n",
            "영어 시퀀스의 최대 길이 11\n",
            "프랑스어 시퀀스의 최대 길이 19\n"
          ]
        }
      ],
      "source": [
        "# 구두점(Punctuation)을 단어와 분리\n",
        "# 프랑스 문법에만 존재하는 whitespace -> 표준 whitespace로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCH_WHITESPACE, r' ', regex=True)\n",
        "#프랑스 문법에만 존재하는 ’를 '로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCH_APOSTROPHE, r\"'\", regex=True)\n",
        "# 프랑스 문법에만 존재하는 <<, >>를 \"로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCH_DOUBLE_COMMA, r'\"', regex=True)\n",
        "# 프랑스 문법에만 존재하는 —를 -로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCE_MINUS_REGEX, r\"-\", regex=True)\n",
        "\n",
        "# 구분점에 whitespace를 지음\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(PUNCTUATION_REGEX, r' \\g<0> ', regex=True).replace(' +', ' ', regex=True).str.strip()\n",
        "lines[\"eng\"] = lines[\"eng\"].str.replace(PUNCTUATION_REGEX, r' \\g<0> ', regex=True).replace(' +', ' ', regex=True).str.strip()\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(' +', ' ', regex=True)\n",
        "lines[\"eng\"] = lines[\"eng\"].str.replace(' +', ' ', regex=True)\n",
        "\n",
        "lines[\"fra_decoder_input\"] = f'{SOS_TOKEN} '+ lines[\"fra\"]\n",
        "lines[\"fra_decoder_target\"] = lines[\"fra\"] + f' {EOS_TOKEN}'\n",
        "lines[\"fra\"] = f'{SOS_TOKEN} '+ lines[\"fra\"] + f' {EOS_TOKEN}' # 양옆에 문장의 시작과 끝의 테그를 붙인다.\n",
        "\n",
        "lines[\"eng\"] = lines[\"eng\"].str.split()\n",
        "\n",
        "lines[\"fra\"] = lines[\"fra\"].str.split()\n",
        "lines[\"fra_decoder_input\"] = lines[\"fra_decoder_input\"].str.split()\n",
        "lines[\"fra_decoder_target\"] = lines[\"fra_decoder_target\"].str.split()\n",
        "\n",
        "eng_tokenizer = Tokenizer(filters=\"\")  # 문자 단위로 Tokenizer를 생성 \n",
        "eng_tokenizer.fit_on_texts(lines[\"eng\"])\n",
        "encoder_input = eng_tokenizer.texts_to_sequences(lines[\"eng\"])\n",
        "\n",
        "fra_tokenizer = Tokenizer(filters=\"\")\n",
        "fra_tokenizer.fit_on_texts(lines[\"fra\"])\n",
        "\n",
        "decoder_input = fra_tokenizer.texts_to_sequences(lines[\"fra_decoder_input\"])\n",
        "decoder_target = fra_tokenizer.texts_to_sequences(lines[\"fra_decoder_target\"])\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
        "print('영어 단어장의 크기 :', eng_vocab_size)\n",
        "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
        "\n",
        "max_eng_seq_len = max(map(len, encoder_input))\n",
        "max_fra_seq_len = max(map(len, decoder_input))\n",
        "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
        "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어문장와 프랑스문장를 각각 tokenize한 뒤<br>\n",
        "단어 사전의크기, 한 문장의 최대 단어 갯수를 확인<br>\n",
        "이 데이터는 후에 padding에 사용할 예정이다.<br>\n",
        "문장의 끝 토큰이 제거된 input과<br>\n",
        "문장의 시작 토큰이 제거된 output을 각각 데이터 프레임에서 생산하였으며<br>\n",
        "이번에는 구두점도 임배딩에 필요하므로 필터옵션을 통해 없어지지 않도록 설정하였다.<br>"
      ],
      "metadata": {
        "id": "ajtkCZfqI5ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
        "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
        "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
        "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
        "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX-uCKCAK1FQ",
        "outputId": "f9218588-2ffb-45bf-dbd8-b09fe42056b4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 데이터의 크기(shape) : (55000, 11)\n",
            "프랑스어 입력데이터의 크기(shape) : (55000, 19)\n",
            "프랑스어 출력데이터의 크기(shape) : (55000, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 언어의 최대 단어갯수에 맞춰 padding을 생성하였다.<br>\n",
        "(`문장의 갯수`,`각 언어의 문장당 단어 최대갯수`)모양의 데이터가 생성되었다.\n"
      ],
      "metadata": {
        "id": "D9kNM2TiK1RF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HbFIKeNV_L8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc6ee5e-adda-4e32-90c0-ba56ace5009f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 14   2  13 597   1   0   0   0   0   0   0]\n",
            "[   1   19   52 1640    3    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[  19   52 1640    3    2    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "shuffle_idx  = np.arange(MAX_SAMPLE_LEN)\n",
        "np.random.shuffle(shuffle_idx)\n",
        "\n",
        "encoder_input = encoder_input[shuffle_idx]\n",
        "decoder_input = decoder_input[shuffle_idx]\n",
        "decoder_target = decoder_target[shuffle_idx]\n",
        "\n",
        "print(encoder_input[3])\n",
        "print(decoder_input[3])\n",
        "print(decoder_target[3])\n",
        "\n",
        "encoder_input_train = encoder_input[:-VALID_LEN]\n",
        "decoder_input_train = decoder_input[:-VALID_LEN]\n",
        "decoder_target_train = decoder_target[:-VALID_LEN]\n",
        "\n",
        "encoder_input_test = encoder_input[-VALID_LEN:]\n",
        "decoder_input_test = decoder_input[-VALID_LEN:]\n",
        "decoder_target_test = decoder_target[-VALID_LEN:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQlTE3xms-k0"
      },
      "source": [
        "과적합 방지를 위해 학습과 점증을 10:1비율로 나누었다.<br>\n",
        "문장셈플은 인덱스를 렌덤값을 통해 섞어서 순서를 바꿨으며<br>\n",
        "임의로 하나의 값을 뽑았을 시<br>\n",
        "`decoder_input`과 `decoder_target`의 값은 `<sos>`, `<eos>`의 차이이므로 올바르게 섞인것을 확인할 수 있다.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu9puDTo_MRT"
      },
      "source": [
        "### 모델 설계"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 과학습을 하지 않는 모델(이론)"
      ],
      "metadata": {
        "id": "XDQWHvL74DO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2qbNBHBm0LLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29141f3b-159e-45e3-e8b2-49e02a9672ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"seq2seq_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, None, 64)     408704      ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, None, 64)     732800      ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " masking_4 (Masking)            (None, None, 64)     0           ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " masking_5 (Masking)            (None, None, 64)     0           ['embedding_5[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)                  [(None, 64),         33024       ['masking_4[0][0]']              \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)                  [(None, None, 64),   33024       ['masking_5[0][0]',              \n",
            "                                 (None, 64),                      'lstm_4[0][1]',                 \n",
            "                                 (None, 64)]                      'lstm_4[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, None, 11450)  744250      ['lstm_5[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,951,802\n",
            "Trainable params: 1,951,802\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embadding = Embedding(eng_vocab_size, HIDDEN_STATE_NUM)\n",
        "encoder_masking = Masking(mask_value=0.0)(encoder_embadding(encoder_inputs))\n",
        "encoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_state = True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_masking)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "decoder_embadding = Embedding(fra_vocab_size, HIDDEN_STATE_NUM)\n",
        "decoder_masking = Masking(mask_value=0.0)(decoder_embadding(decoder_inputs))\n",
        "decoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_sequences = True, return_state=True)\n",
        "decoder_outputs, _, _= decoder_lstm(decoder_masking, initial_state = encoder_states)\n",
        "\n",
        "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"seq2seq_model\")\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    , loss=\"sparse_categorical_crossentropy\", metrics=['acc']\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "encorder는 영어를 프랑스어로 번역하기 위한 레이어이므로<br>\n",
        "decorder의 input에 맞게 모델을 구성하였으며<br>\n",
        "one-hot 인코딩을 하지 않은 상태이므로 `sparse_categorical_crossentropy`로 설정하였다."
      ],
      "metadata": {
        "id": "yDMdVE5Grafh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 과학습을 하는 모델(실제)"
      ],
      "metadata": {
        "id": "W-F9UWIh4bbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overfit_encoder_inputs = Input(shape=(None,))\n",
        "overfit_encoder_embadding = Embedding(eng_vocab_size, HIDDEN_STATE_NUM)\n",
        "overfit_encoder_masking = Masking(mask_value=0.0)(overfit_encoder_embadding(overfit_encoder_inputs))\n",
        "overfit_encoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_state = True)\n",
        "overfit_encoder_outputs, overfit_state_h, overfit_state_c = overfit_encoder_lstm(overfit_encoder_masking)\n",
        "overfit_encoder_states = [overfit_state_h, overfit_state_c]\n",
        "\n",
        "overfit_decoder_inputs = Input(shape=(None, ))\n",
        "overfit_decoder_embadding = Embedding(fra_vocab_size, HIDDEN_STATE_NUM)\n",
        "overfit_decoder_masking = Masking(mask_value=0.0)(overfit_decoder_embadding(overfit_decoder_inputs))\n",
        "overfit_decoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_sequences = True, return_state=True)\n",
        "overfit_decoder_outputs, _, _= overfit_decoder_lstm(overfit_decoder_masking, initial_state = overfit_encoder_states)\n",
        "\n",
        "overfit_decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
        "overfit_decoder_outputs = overfit_decoder_softmax_layer(overfit_decoder_outputs)\n",
        "\n",
        "overfit_model = Model([overfit_encoder_inputs, overfit_decoder_inputs], overfit_decoder_outputs, name=\"overfit_seq2seq_model\")\n",
        "overfit_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.005)\n",
        "    , loss=\"sparse_categorical_crossentropy\", metrics=['acc']\n",
        ")\n",
        "overfit_model.summary()"
      ],
      "metadata": {
        "id": "Op3kYHQE4Xcu",
        "outputId": "13d188b6-20a8-4cab-c07b-ade71661481a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"overfit_seq2seq_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)        (None, None, 64)     408704      ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)        (None, None, 64)     732800      ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " masking_6 (Masking)            (None, None, 64)     0           ['embedding_6[0][0]']            \n",
            "                                                                                                  \n",
            " masking_7 (Masking)            (None, None, 64)     0           ['embedding_7[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_6 (LSTM)                  [(None, 64),         33024       ['masking_6[0][0]']              \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " lstm_7 (LSTM)                  [(None, None, 64),   33024       ['masking_7[0][0]',              \n",
            "                                 (None, 64),                      'lstm_6[0][1]',                 \n",
            "                                 (None, 64)]                      'lstm_6[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, None, 11450)  744250      ['lstm_7[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,951,802\n",
            "Trainable params: 1,951,802\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전의 과학습 아닌 모델과 다른 점은 optimizer부분에 running rate의 차이점밖에 없다."
      ],
      "metadata": {
        "id": "nRrqhFjb5sZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습"
      ],
      "metadata": {
        "id": "ajgB-k1RFH1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit_kwargs[\"validation_data\"] = ([encoder_input_test, decoder_input_test], decoder_target_test)\n",
        "\n",
        "history_dict = model.fit(\n",
        "                         x=[encoder_input_train, decoder_input_train]\n",
        "                         , y=decoder_target_train\n",
        "                         , **fit_kwargs).history\n",
        "\n",
        "overfit_history_dict = overfit_model.fit(\n",
        "                            x=[encoder_input_train, decoder_input_train]\n",
        "                            , y=decoder_target_train\n",
        "                            , **fit_kwargs).history\n",
        "\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "overfit_loss = overfit_history_dict['loss']\n",
        "overfit_val_loss = overfit_history_dict['val_loss']\n",
        "\n",
        "epoch = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epoch, loss, 'bo', label='Training loss')\n",
        "plt.plot(epoch, val_loss, 'b', label='Validation loss')\n",
        "plt.plot(epoch, loss, 'ro', label='Overfit training loss')\n",
        "plt.plot(epoch, val_loss, 'r', label='Overfit validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "hy1iw1_slsf3",
        "outputId": "e876085e-e836-4f28-9bff-196314a87ddf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "168/782 [=====>........................] - ETA: 15s - loss: 8.8327 - acc: 0.5666"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5c10a05585b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_target_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                          , **fit_kwargs).history\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m overfit_history_dict = overfit_model.fit(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m       \u001b[0mio_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_break\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# request flush on the background thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0;31m# wait for flush to actually get through, if we can.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;31m# waiting across threads during import can cause deadlocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     def send_multipart(\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNQ9W93B0LUx"
      },
      "source": [
        "입력값은 똑같이 적용하였으며<br>\n",
        "학습결과\n",
        "- overfit이 되지않는 모델의 경우는 정상적으로 우하향하는 그래프의 모습을 보이며<br>train loss, validation loss 의 차이가 크지않다.\n",
        "- overfit이 된 모델은 대략 epoch이 8 이후부터 이상점이 드러나며<br>\n",
        "어느 기점으로 validation loss의 값이 증가하는 모습을 볼 수 있다.\n",
        "\n",
        "라는 결과를 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 검증\n"
      ],
      "metadata": {
        "id": "mv2-aMcEvAs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################<<<nomal>>>##################################################################\n",
        "\n",
        "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
        "encoder_model.summary()\n",
        "\n",
        "# 이전 time step의 hidden state를 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 cell state를 저장하는 텐서\n",
        "decoder_state_input_c = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_embadding_val = decoder_embadding(decoder_inputs)\n",
        "\n",
        "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
        "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
        "decoder_outputs_val, state_h, state_c = decoder_lstm(decoder_embadding_val, initial_state = decoder_states_inputs)\n",
        "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
        "decoder_states_val = [state_h, state_c]\n",
        "\n",
        "decoder_outputs_val = decoder_softmax_layer(decoder_outputs_val)\n",
        "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs_val] + decoder_states_val, name=\"teaching_model\")\n",
        "decoder_model.summary()\n",
        "\n",
        "###################################################<<<overfit>>>##################################################################\n",
        "\n",
        "overfit_encoder_model = Model(inputs = overfit_encoder_inputs, outputs = overfit_encoder_states)\n",
        "overfit_encoder_model.summary()\n",
        "\n",
        "# 이전 time step의 hidden state를 저장하는 텐서\n",
        "overfit_decoder_state_input_h = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 cell state를 저장하는 텐서\n",
        "overfit_decoder_state_input_c = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
        "overfit_decoder_states_inputs = [overfit_decoder_state_input_h, overfit_decoder_state_input_c]\n",
        "\n",
        "overfit_decoder_embadding_val = overfit_decoder_embadding(overfit_decoder_inputs)\n",
        "\n",
        "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
        "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
        "overfit_decoder_outputs_val, overfit_state_h, overfit_state_c = overfit_decoder_lstm(overfit_decoder_embadding_val, initial_state = overfit_decoder_states_inputs)\n",
        "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
        "overfit_decoder_states_val = [overfit_state_h, overfit_state_c]\n",
        "\n",
        "overfit_decoder_outputs_val = overfit_decoder_softmax_layer(overfit_decoder_outputs_val)\n",
        "overfit_decoder_model = Model(inputs=[overfit_decoder_inputs] + overfit_decoder_states_inputs, outputs=[overfit_decoder_outputs_val] + overfit_decoder_states_val, name=\"overfit_teaching_model\")\n",
        "overfit_decoder_model.summary()"
      ],
      "metadata": {
        "id": "BEZRY1YZvA39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습이 과적합된 모델 혹은 아닌모델 두 경우의 teaching forcing을 구현하였다."
      ],
      "metadata": {
        "id": "6_PrcpfDvBDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng2idx = eng_tokenizer.word_index\n",
        "fra2idx = fra_tokenizer.word_index\n",
        "idx2eng = eng_tokenizer.index_word\n",
        "idx2fra = fra_tokenizer.index_word"
      ],
      "metadata": {
        "id": "m1Soi1Am0_wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 -> 인덱스, 인덱스 -> 단어에 해당하는 데이터 생성"
      ],
      "metadata": {
        "id": "9Q_oqPgi0_5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = fra2idx[SOS_TOKEN]\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    # stop_condition이 True가 될 때까지 루프 반복\n",
        "    while not stop_condition:\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # 예측 결과를 문자로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = idx2fra[sampled_token_index]\n",
        "\n",
        "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
        "        decoded_sentence += \" \" + sampled_char\n",
        "\n",
        "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "        if (sampled_char == EOS_TOKEN or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "wPvlKpzY1B5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "예제에 있던 코드를 가져왔으며 ont-hot vector에 해당하는 부분을 입/출력값에 맞게 조정하였다.<br>\n",
        "이번에는 과적합 여부에 관련된 모델의 출력값을 비교가 필요하므로<br>\n",
        "각 모델의 조건에 따라 결과값을 출력하는 함수로 변경하였다.<br>"
      ],
      "metadata": {
        "id": "EXgwX_Ff1CEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 10\n",
        "input_seq = encoder_input_train[sample: sample + 1]\n",
        "\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"입력문장 :\", \"\".join([\n",
        "        idx2eng[encoded_word] + \" \"\n",
        "        for encoded_word in encoder_input_train[sample]\n",
        "        if encoded_word != 0\n",
        "    ])\n",
        ")\n",
        "print(\"정답문장 :\",  \"\".join([\n",
        "      idx2fra[encoded_word] + \" \"\n",
        "      for encoded_word in decoder_input_train[sample]\n",
        "      if encoded_word != 0 and encoded_word != fra2idx[SOS_TOKEN] and encoded_word != fra2idx[EOS_TOKEN]\n",
        "  ])\n",
        ")\n",
        "\n",
        "print(\"과적합이 안된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, encoder_model, decoder_model))\n",
        "print(\"과적합된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, overfit_encoder_model, overfit_decoder_model))\n",
        "\n",
        "print(\"-\" * 35)\n",
        "\n",
        "input_seq = encoder_input_test[sample: sample + 1]\n",
        "\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"입력문장 :\", \"\".join([\n",
        "        idx2eng[encoded_word] + \" \"\n",
        "        for encoded_word in encoder_input_test[sample]\n",
        "        if encoded_word != 0\n",
        "    ])\n",
        ")\n",
        "print(\"정답문장 :\",  \"\".join([\n",
        "      idx2fra[encoded_word] + \" \"\n",
        "      for encoded_word in decoder_input_test[sample]\n",
        "      if encoded_word != 0 and encoded_word != fra2idx[SOS_TOKEN] and encoded_word != fra2idx[EOS_TOKEN]\n",
        "  ])\n",
        ")\n",
        "\n",
        "print(\"과적합이 안된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, encoder_model, decoder_model))\n",
        "print(\"과적합된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, overfit_encoder_model, overfit_decoder_model))\n",
        "\n",
        "print(\"-\" * 35)"
      ],
      "metadata": {
        "id": "S5eU9KvT1XZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E7-ju2Gp1Xko"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsx8OVYGnM-s"
      },
      "source": [
        "### 회고\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ngdabGo3j4l"
      },
      "source": [
        "- 학습된 데이터가 전체적으로 높은 정확도를 보여 미리 학습된 데이터의 중요성을 알게되었다.\n",
        "- 항상 학습된 데이터가 높은 정확도를 나타내는 것이 아니라 하이퍼파라미터의 튜닝에 따라 결과가 달라질 수 있다는 것을 알게 되었다.\n",
        "\n",
        "※ 이번 레포트는 양희성님의 모델구조의 조언으로 작성되었음을 알려드립니다.<br>\n",
        "   희성님에게 감사하다는 글을 남기며 이만 글을 마치겠습니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}