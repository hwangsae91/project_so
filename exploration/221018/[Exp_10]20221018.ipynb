{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangsae91/project_so/blob/master/exploration/221018/%5BExp_10%5D20221018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "302ZdfCKLlhI",
        "outputId": "d713129c-f973-4e6d-d95c-8db8b46137ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# google colab전용\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujMb2MtyLrVQ"
      },
      "source": [
        "# exploration 10번째 과제\n",
        "@ 황한용(3기/쏘카)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdrfR8Y3L0ry"
      },
      "source": [
        "## 라이브러리 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LWLP8EFSLqqp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import string # 구두점 정규화 표현식\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`tensorflow.keras.preprocessing.sequence.pad_sequences`는 모듈 위치가 변경되었으므로\n",
        "`tf.keras.utils.pad_sequences`로 변경<br>"
      ],
      "metadata": {
        "id": "dwTie8y2eOO0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qC6hZWKL7x3"
      },
      "source": [
        "## 상수선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XcIjEEgnMD75"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/data/translator_seq2seq\" # 데이터 기본경로\n",
        "DATA_PATH  = BASE_PATH + \"/data/fra.txt\" # 사전 기본\n",
        "\n",
        "SOS_TOKEN = '<sos>' # 문장 시작토큰\n",
        "EOS_TOKEN = '<eos>' # 문장 끝 토큰\n",
        "\n",
        "MAX_SAMPLE_LEN = 88000 # 최대 단어갯수\n",
        "VALID_LEN = 8000 # 검증할 문장의 갯수\n",
        "PUNCTUATION_REGEX = r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]' # 정규화\n",
        "FRENCH_WHITESPACE = r'[\\xa0\\u202f\\u2009]' # whitespace 정규화(1/4 whitespace 등의 대응)\n",
        "FRENCH_APOSTROPHE = r\"’\" # 프랑스어 '\n",
        "FRENCH_DOUBLE_COMMA = r\"'<<|>>'\" # 프랑스어 \"\n",
        "FRENCE_MINUS_REGEX = r\"\\—\" # 프랑스어 -\n",
        "\n",
        "HIDDEN_STATE_NUM = 256 # hidden state의 노드수\n",
        "\n",
        "fit_kwargs = {\n",
        "    \"epochs\":50 # epoch 횟수\n",
        "    , \"batch_size\":512\n",
        "    ,\"validation_data\": None # 추후 추가예정\n",
        "    , \"shuffle\" : True # epoch당 셔플을 할지의 여부\n",
        "    , \"verbose\":1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어의 경우는 `string.punctuation`의 구두점을 활용하여 만들었다.<br>\n",
        "프랑스어의 경우는 문법에 따라 1/8, 1/4, 1/2 `whitespace`를 표준`whitespace`로 변경하였으며<br>\n",
        "`'`, `\"`, `-` 또한 표준으로 변경하였다.<br>\n",
        "알파벳의 악쌍(악센트, 성조)은 단어의 의미를 결정하는 요소이므로 제거하지 않았다."
      ],
      "metadata": {
        "id": "NtHICg5lZmae"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5DV1kd8MPes"
      },
      "source": [
        "## 메인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7XCWajZ0XIj"
      },
      "source": [
        "### 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwkvcfr7MPsm",
        "outputId": "d152cdc9-e354-4cfa-d71e-d292cd5cea23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플의 수 : 197463\n"
          ]
        }
      ],
      "source": [
        "lines = pd.read_csv(DATA_PATH, names=['eng', 'fra', 'cc'], sep='\\t')\n",
        "print('전체 샘플의 수 :',len(lines))\n",
        "lines.sample(5) #샘플 5개 출력\n",
        "lines.pop('cc')\n",
        "lines = lines.head(MAX_SAMPLE_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXm0KjWe845_"
      },
      "source": [
        "데이터에 대한 설명은 아래와 같다.</br>\n",
        "- eng: 영어문장\n",
        "- fra: 영어 문장에 해당되는 프랑스 문장\n",
        "- cc: 저작권 정보\n",
        "\n",
        "저작권 정보는 데이터 분석에 사용되지 않으므로 로드하지 않았다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "971TB3kw845_",
        "outputId": "ca79eaa3-8b09-4c2e-be92-0a4fef4bfb47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 단어장의 크기 : 8329\n",
            "프랑스어 단어장의 크기 : 14702\n",
            "영어 시퀀스의 최대 길이 12\n",
            "프랑스어 시퀀스의 최대 길이 19\n"
          ]
        }
      ],
      "source": [
        "# 구두점(Punctuation)을 단어와 분리\n",
        "# 프랑스 문법에만 존재하는 whitespace -> 표준 whitespace로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCH_WHITESPACE, r' ', regex=True)\n",
        "#프랑스 문법에만 존재하는 ’를 '로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCH_APOSTROPHE, r\"'\", regex=True)\n",
        "# 프랑스 문법에만 존재하는 <<, >>를 \"로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCH_DOUBLE_COMMA, r'\"', regex=True)\n",
        "# 프랑스 문법에만 존재하는 —를 -로 변경\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(FRENCE_MINUS_REGEX, r\"-\", regex=True)\n",
        "\n",
        "# 구분점에 whitespace를 지음\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(PUNCTUATION_REGEX, r' \\g<0> ', regex=True).replace(' +', ' ', regex=True).str.strip()\n",
        "lines[\"eng\"] = lines[\"eng\"].str.replace(PUNCTUATION_REGEX, r' \\g<0> ', regex=True).replace(' +', ' ', regex=True).str.strip()\n",
        "lines[\"fra\"] = lines[\"fra\"].str.replace(' +', ' ', regex=True)\n",
        "lines[\"eng\"] = lines[\"eng\"].str.replace(' +', ' ', regex=True)\n",
        "\n",
        "lines[\"fra_decoder_input\"] = f'{SOS_TOKEN} '+ lines[\"fra\"]\n",
        "lines[\"fra_decoder_target\"] = lines[\"fra\"] + f' {EOS_TOKEN}'\n",
        "lines[\"fra\"] = f'{SOS_TOKEN} '+ lines[\"fra\"] + f' {EOS_TOKEN}' # 양옆에 문장의 시작과 끝의 테그를 붙인다.\n",
        "\n",
        "lines[\"eng\"] = lines[\"eng\"].str.split()\n",
        "\n",
        "lines[\"fra\"] = lines[\"fra\"].str.split()\n",
        "lines[\"fra_decoder_input\"] = lines[\"fra_decoder_input\"].str.split()\n",
        "lines[\"fra_decoder_target\"] = lines[\"fra_decoder_target\"].str.split()\n",
        "\n",
        "eng_tokenizer = Tokenizer(filters=\"\")  # 문자 단위로 Tokenizer를 생성 \n",
        "eng_tokenizer.fit_on_texts(lines[\"eng\"])\n",
        "encoder_input = eng_tokenizer.texts_to_sequences(lines[\"eng\"])\n",
        "\n",
        "fra_tokenizer = Tokenizer(filters=\"\")\n",
        "fra_tokenizer.fit_on_texts(lines[\"fra\"])\n",
        "\n",
        "decoder_input = fra_tokenizer.texts_to_sequences(lines[\"fra_decoder_input\"])\n",
        "decoder_target = fra_tokenizer.texts_to_sequences(lines[\"fra_decoder_target\"])\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
        "print('영어 단어장의 크기 :', eng_vocab_size)\n",
        "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
        "\n",
        "max_eng_seq_len = max(map(len, encoder_input))\n",
        "max_fra_seq_len = max(map(len, decoder_input))\n",
        "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
        "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어문장와 프랑스문장를 각각 tokenize한 뒤<br>\n",
        "단어 사전의크기, 한 문장의 최대 단어 갯수를 확인<br>\n",
        "이 데이터는 후에 padding에 사용할 예정이다.<br>\n",
        "문장의 끝 토큰이 제거된 input과<br>\n",
        "문장의 시작 토큰이 제거된 output을 각각 데이터 프레임에서 생산하였으며<br>\n",
        "이번에는 구두점도 임배딩에 필요하므로 필터옵션을 통해 없어지지 않도록 설정하였다.<br>"
      ],
      "metadata": {
        "id": "ajtkCZfqI5ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
        "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
        "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
        "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
        "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX-uCKCAK1FQ",
        "outputId": "177697c1-bef7-44e5-e2bf-b3a7a0f153a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 데이터의 크기(shape) : (88000, 12)\n",
            "프랑스어 입력데이터의 크기(shape) : (88000, 19)\n",
            "프랑스어 출력데이터의 크기(shape) : (88000, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 언어의 최대 단어갯수에 맞춰 padding을 생성하였다.<br>\n",
        "(`문장의 갯수`,`각 언어의 문장당 단어 최대갯수`)모양의 데이터가 생성되었다.\n"
      ],
      "metadata": {
        "id": "D9kNM2TiK1RF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HbFIKeNV_L8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3116051-171c-456d-c1ab-40d55aa2317f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  30  583 1913    1    0    0    0    0    0    0    0    0]\n",
            "[   1   67   65   50 2376 5558    3    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[  67   65   50 2376 5558    3    2    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "shuffle_idx  = np.arange(MAX_SAMPLE_LEN)\n",
        "np.random.shuffle(shuffle_idx)\n",
        "\n",
        "encoder_input = encoder_input[shuffle_idx]\n",
        "decoder_input = decoder_input[shuffle_idx]\n",
        "decoder_target = decoder_target[shuffle_idx]\n",
        "\n",
        "print(encoder_input[3])\n",
        "print(decoder_input[3])\n",
        "print(decoder_target[3])\n",
        "\n",
        "encoder_input_train = encoder_input[:-VALID_LEN]\n",
        "decoder_input_train = decoder_input[:-VALID_LEN]\n",
        "decoder_target_train = decoder_target[:-VALID_LEN]\n",
        "\n",
        "encoder_input_test = encoder_input[-VALID_LEN:]\n",
        "decoder_input_test = decoder_input[-VALID_LEN:]\n",
        "decoder_target_test = decoder_target[-VALID_LEN:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQlTE3xms-k0"
      },
      "source": [
        "과적합 방지를 위해 학습과 점증을 10:1비율로 나누었다.<br>\n",
        "문장셈플은 인덱스를 렌덤값을 통해 섞어서 순서를 바꿨으며<br>\n",
        "임의로 하나의 값을 뽑았을 시<br>\n",
        "`decoder_input`과 `decoder_target`의 값은 `<sos>`, `<eos>`의 차이이므로 올바르게 섞인것을 확인할 수 있다.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu9puDTo_MRT"
      },
      "source": [
        "### 모델 설계"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 과학습을 하지 않는 모델(이론)"
      ],
      "metadata": {
        "id": "XDQWHvL74DO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2qbNBHBm0LLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968f5c80-57db-441e-bc68-db5b2b587614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"seq2seq_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 256)    2132224     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    3763712     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " masking (Masking)              (None, None, 256)    0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " masking_1 (Masking)            (None, None, 256)    0           ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        525312      ['masking[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  525312      ['masking_1[0][0]',              \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 14702)  3778414     ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,724,974\n",
            "Trainable params: 10,724,974\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embadding = Embedding(eng_vocab_size, HIDDEN_STATE_NUM)\n",
        "encoder_masking = Masking(mask_value=0.0)(encoder_embadding(encoder_inputs))\n",
        "encoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_state = True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_masking)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "decoder_embadding = Embedding(fra_vocab_size, HIDDEN_STATE_NUM)\n",
        "decoder_masking = Masking(mask_value=0.0)(decoder_embadding(decoder_inputs))\n",
        "decoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_sequences = True, return_state=True)\n",
        "decoder_outputs, _, _= decoder_lstm(decoder_masking, initial_state = encoder_states)\n",
        "\n",
        "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"seq2seq_model\")\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    , loss=\"sparse_categorical_crossentropy\", metrics=['acc']\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "encorder는 영어를 프랑스어로 번역하기 위한 레이어이므로<br>\n",
        "decorder의 input에 맞게 모델을 구성하였으며<br>\n",
        "one-hot 인코딩을 하지 않은 상태이므로 `sparse_categorical_crossentropy`로 설정하였다."
      ],
      "metadata": {
        "id": "yDMdVE5Grafh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 과학습을 하는 모델(실제)"
      ],
      "metadata": {
        "id": "W-F9UWIh4bbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overfit_encoder_inputs = Input(shape=(None,))\n",
        "overfit_encoder_embadding = Embedding(eng_vocab_size, HIDDEN_STATE_NUM)\n",
        "overfit_encoder_masking = Masking(mask_value=0.0)(overfit_encoder_embadding(overfit_encoder_inputs))\n",
        "overfit_encoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_state = True)\n",
        "overfit_encoder_outputs, overfit_state_h, overfit_state_c = overfit_encoder_lstm(overfit_encoder_masking)\n",
        "overfit_encoder_states = [overfit_state_h, overfit_state_c]\n",
        "\n",
        "overfit_decoder_inputs = Input(shape=(None, ))\n",
        "overfit_decoder_embadding = Embedding(fra_vocab_size, HIDDEN_STATE_NUM)\n",
        "overfit_decoder_masking = Masking(mask_value=0.0)(overfit_decoder_embadding(overfit_decoder_inputs))\n",
        "overfit_decoder_lstm = LSTM(units=HIDDEN_STATE_NUM, return_sequences = True, return_state=True)\n",
        "overfit_decoder_outputs, _, _= overfit_decoder_lstm(overfit_decoder_masking, initial_state = overfit_encoder_states)\n",
        "\n",
        "overfit_decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
        "overfit_decoder_outputs = overfit_decoder_softmax_layer(overfit_decoder_outputs)\n",
        "\n",
        "overfit_model = Model([overfit_encoder_inputs, overfit_decoder_inputs], overfit_decoder_outputs, name=\"overfit_seq2seq_model\")\n",
        "overfit_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.005)\n",
        "    , loss=\"sparse_categorical_crossentropy\", metrics=['acc']\n",
        ")\n",
        "overfit_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op3kYHQE4Xcu",
        "outputId": "640a29e2-03b8-4f2c-db0b-1059a40f6dcc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"overfit_seq2seq_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, None, 256)    2132224     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 256)    3763712     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " masking_2 (Masking)            (None, None, 256)    0           ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " masking_3 (Masking)            (None, None, 256)    0           ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 256),        525312      ['masking_2[0][0]']              \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['masking_3[0][0]',              \n",
            "                                 (None, 256),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 14702)  3778414     ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,724,974\n",
            "Trainable params: 10,724,974\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전의 과학습 아닌 모델과 다른 점은 optimizer부분에 running rate의 차이점밖에 없다."
      ],
      "metadata": {
        "id": "nRrqhFjb5sZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습"
      ],
      "metadata": {
        "id": "ajgB-k1RFH1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit_kwargs[\"validation_data\"] = ([encoder_input_test, decoder_input_test], decoder_target_test)\n",
        "\n",
        "history_dict = model.fit(\n",
        "                         x=[encoder_input_train, decoder_input_train]\n",
        "                         , y=decoder_target_train\n",
        "                         , **fit_kwargs).history\n",
        "\n",
        "overfit_history_dict = overfit_model.fit(\n",
        "                            x=[encoder_input_train, decoder_input_train]\n",
        "                            , y=decoder_target_train\n",
        "                            , **fit_kwargs).history\n",
        "\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "overfit_loss = overfit_history_dict['loss']\n",
        "overfit_val_loss = overfit_history_dict['val_loss']\n",
        "\n",
        "epoch = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epoch, loss, 'bo', label='Training loss')\n",
        "plt.plot(epoch, val_loss, 'b', label='Validation loss')\n",
        "plt.plot(epoch, loss, 'ro', label='Overfit training loss')\n",
        "plt.plot(epoch, val_loss, 'r', label='Overfit validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hy1iw1_slsf3",
        "outputId": "c501b5c2-d060-4ac1-f874-abe3617f5532"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "157/157 [==============================] - 34s 162ms/step - loss: 6.0822 - acc: 0.5759 - val_loss: 2.8078 - val_acc: 0.5935\n",
            "Epoch 2/50\n",
            "157/157 [==============================] - 23s 144ms/step - loss: 2.5361 - acc: 0.5946 - val_loss: 2.4081 - val_acc: 0.5935\n",
            "Epoch 3/50\n",
            "157/157 [==============================] - 23s 144ms/step - loss: 2.3301 - acc: 0.5946 - val_loss: 2.2821 - val_acc: 0.5935\n",
            "Epoch 4/50\n",
            "157/157 [==============================] - 23s 145ms/step - loss: 2.2086 - acc: 0.6064 - val_loss: 2.1672 - val_acc: 0.6172\n",
            "Epoch 5/50\n",
            "157/157 [==============================] - 23s 146ms/step - loss: 2.1113 - acc: 0.6336 - val_loss: 2.0843 - val_acc: 0.6421\n",
            "Epoch 6/50\n",
            "157/157 [==============================] - 23s 146ms/step - loss: 2.0345 - acc: 0.6623 - val_loss: 2.0116 - val_acc: 0.6897\n",
            "Epoch 7/50\n",
            "157/157 [==============================] - 23s 146ms/step - loss: 1.9642 - acc: 0.7047 - val_loss: 1.9428 - val_acc: 0.7093\n",
            "Epoch 8/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.8968 - acc: 0.7115 - val_loss: 1.8779 - val_acc: 0.7113\n",
            "Epoch 9/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.8355 - acc: 0.7132 - val_loss: 1.8188 - val_acc: 0.7154\n",
            "Epoch 10/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.7756 - acc: 0.7208 - val_loss: 1.7571 - val_acc: 0.7266\n",
            "Epoch 11/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.7136 - acc: 0.7324 - val_loss: 1.6984 - val_acc: 0.7351\n",
            "Epoch 12/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.6575 - acc: 0.7379 - val_loss: 1.6470 - val_acc: 0.7388\n",
            "Epoch 13/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.6074 - acc: 0.7414 - val_loss: 1.6004 - val_acc: 0.7423\n",
            "Epoch 14/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.5617 - acc: 0.7458 - val_loss: 1.5581 - val_acc: 0.7483\n",
            "Epoch 15/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.5193 - acc: 0.7540 - val_loss: 1.5181 - val_acc: 0.7581\n",
            "Epoch 16/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.4796 - acc: 0.7633 - val_loss: 1.4816 - val_acc: 0.7661\n",
            "Epoch 17/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.4439 - acc: 0.7699 - val_loss: 1.4495 - val_acc: 0.7700\n",
            "Epoch 18/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.4123 - acc: 0.7736 - val_loss: 1.4209 - val_acc: 0.7733\n",
            "Epoch 19/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.3839 - acc: 0.7769 - val_loss: 1.3947 - val_acc: 0.7771\n",
            "Epoch 20/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.3576 - acc: 0.7807 - val_loss: 1.3709 - val_acc: 0.7800\n",
            "Epoch 21/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.3330 - acc: 0.7833 - val_loss: 1.3488 - val_acc: 0.7832\n",
            "Epoch 22/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.3101 - acc: 0.7856 - val_loss: 1.3286 - val_acc: 0.7843\n",
            "Epoch 23/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.2891 - acc: 0.7878 - val_loss: 1.3102 - val_acc: 0.7872\n",
            "Epoch 24/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.2692 - acc: 0.7908 - val_loss: 1.2921 - val_acc: 0.7905\n",
            "Epoch 25/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.2499 - acc: 0.7934 - val_loss: 1.2746 - val_acc: 0.7926\n",
            "Epoch 26/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.2311 - acc: 0.7959 - val_loss: 1.2575 - val_acc: 0.7950\n",
            "Epoch 27/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.2123 - acc: 0.7984 - val_loss: 1.2399 - val_acc: 0.7976\n",
            "Epoch 28/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.1937 - acc: 0.8009 - val_loss: 1.2242 - val_acc: 0.7994\n",
            "Epoch 29/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.1758 - acc: 0.8032 - val_loss: 1.2086 - val_acc: 0.8016\n",
            "Epoch 30/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.1588 - acc: 0.8054 - val_loss: 1.1938 - val_acc: 0.8038\n",
            "Epoch 31/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.1417 - acc: 0.8077 - val_loss: 1.1784 - val_acc: 0.8058\n",
            "Epoch 32/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.1253 - acc: 0.8098 - val_loss: 1.1653 - val_acc: 0.8071\n",
            "Epoch 33/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.1089 - acc: 0.8118 - val_loss: 1.1504 - val_acc: 0.8091\n",
            "Epoch 34/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.0926 - acc: 0.8139 - val_loss: 1.1368 - val_acc: 0.8104\n",
            "Epoch 35/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.0764 - acc: 0.8159 - val_loss: 1.1212 - val_acc: 0.8128\n",
            "Epoch 36/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.0605 - acc: 0.8177 - val_loss: 1.1078 - val_acc: 0.8142\n",
            "Epoch 37/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.0449 - acc: 0.8197 - val_loss: 1.0952 - val_acc: 0.8155\n",
            "Epoch 38/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.0296 - acc: 0.8216 - val_loss: 1.0813 - val_acc: 0.8176\n",
            "Epoch 39/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.0149 - acc: 0.8233 - val_loss: 1.0689 - val_acc: 0.8192\n",
            "Epoch 40/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 1.0005 - acc: 0.8248 - val_loss: 1.0569 - val_acc: 0.8202\n",
            "Epoch 41/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.9864 - acc: 0.8265 - val_loss: 1.0443 - val_acc: 0.8221\n",
            "Epoch 42/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.9725 - acc: 0.8282 - val_loss: 1.0326 - val_acc: 0.8232\n",
            "Epoch 43/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.9587 - acc: 0.8298 - val_loss: 1.0214 - val_acc: 0.8247\n",
            "Epoch 44/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.9454 - acc: 0.8312 - val_loss: 1.0101 - val_acc: 0.8265\n",
            "Epoch 45/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.9326 - acc: 0.8327 - val_loss: 0.9995 - val_acc: 0.8278\n",
            "Epoch 46/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.9201 - acc: 0.8342 - val_loss: 0.9898 - val_acc: 0.8287\n",
            "Epoch 47/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.9078 - acc: 0.8357 - val_loss: 0.9796 - val_acc: 0.8300\n",
            "Epoch 48/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.8958 - acc: 0.8370 - val_loss: 0.9703 - val_acc: 0.8310\n",
            "Epoch 49/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.8840 - acc: 0.8384 - val_loss: 0.9607 - val_acc: 0.8323\n",
            "Epoch 50/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.8726 - acc: 0.8397 - val_loss: 0.9517 - val_acc: 0.8335\n",
            "Epoch 1/50\n",
            "157/157 [==============================] - 33s 162ms/step - loss: 2.2882 - acc: 0.6706 - val_loss: 1.7198 - val_acc: 0.7480\n",
            "Epoch 2/50\n",
            "157/157 [==============================] - 23s 146ms/step - loss: 1.5027 - acc: 0.7693 - val_loss: 1.3763 - val_acc: 0.7860\n",
            "Epoch 3/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 1.2154 - acc: 0.8022 - val_loss: 1.1290 - val_acc: 0.8142\n",
            "Epoch 4/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.9478 - acc: 0.8311 - val_loss: 0.8665 - val_acc: 0.8422\n",
            "Epoch 5/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.7054 - acc: 0.8605 - val_loss: 0.7017 - val_acc: 0.8660\n",
            "Epoch 6/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.5306 - acc: 0.8858 - val_loss: 0.5944 - val_acc: 0.8816\n",
            "Epoch 7/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.4093 - acc: 0.9047 - val_loss: 0.5368 - val_acc: 0.8912\n",
            "Epoch 8/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.3273 - acc: 0.9186 - val_loss: 0.5021 - val_acc: 0.8978\n",
            "Epoch 9/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.2704 - acc: 0.9296 - val_loss: 0.4860 - val_acc: 0.9015\n",
            "Epoch 10/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.2299 - acc: 0.9379 - val_loss: 0.4758 - val_acc: 0.9032\n",
            "Epoch 11/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.2002 - acc: 0.9442 - val_loss: 0.4722 - val_acc: 0.9053\n",
            "Epoch 12/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1782 - acc: 0.9489 - val_loss: 0.4708 - val_acc: 0.9056\n",
            "Epoch 13/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1597 - acc: 0.9530 - val_loss: 0.4726 - val_acc: 0.9064\n",
            "Epoch 14/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1460 - acc: 0.9560 - val_loss: 0.4750 - val_acc: 0.9062\n",
            "Epoch 15/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1340 - acc: 0.9587 - val_loss: 0.4804 - val_acc: 0.9068\n",
            "Epoch 16/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1242 - acc: 0.9610 - val_loss: 0.4837 - val_acc: 0.9067\n",
            "Epoch 17/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1167 - acc: 0.9627 - val_loss: 0.4879 - val_acc: 0.9076\n",
            "Epoch 18/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1096 - acc: 0.9645 - val_loss: 0.4948 - val_acc: 0.9063\n",
            "Epoch 19/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1040 - acc: 0.9656 - val_loss: 0.5012 - val_acc: 0.9072\n",
            "Epoch 20/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0995 - acc: 0.9667 - val_loss: 0.5070 - val_acc: 0.9064\n",
            "Epoch 21/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0955 - acc: 0.9675 - val_loss: 0.5112 - val_acc: 0.9069\n",
            "Epoch 22/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0926 - acc: 0.9682 - val_loss: 0.5172 - val_acc: 0.9063\n",
            "Epoch 23/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0899 - acc: 0.9688 - val_loss: 0.5220 - val_acc: 0.9069\n",
            "Epoch 24/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0872 - acc: 0.9695 - val_loss: 0.5321 - val_acc: 0.9062\n",
            "Epoch 25/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0852 - acc: 0.9700 - val_loss: 0.5322 - val_acc: 0.9062\n",
            "Epoch 26/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0832 - acc: 0.9704 - val_loss: 0.5377 - val_acc: 0.9061\n",
            "Epoch 27/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0818 - acc: 0.9706 - val_loss: 0.5412 - val_acc: 0.9057\n",
            "Epoch 28/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0814 - acc: 0.9707 - val_loss: 0.5477 - val_acc: 0.9056\n",
            "Epoch 29/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0803 - acc: 0.9710 - val_loss: 0.5545 - val_acc: 0.9058\n",
            "Epoch 30/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0797 - acc: 0.9711 - val_loss: 0.5593 - val_acc: 0.9053\n",
            "Epoch 31/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0784 - acc: 0.9713 - val_loss: 0.5624 - val_acc: 0.9051\n",
            "Epoch 32/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0766 - acc: 0.9718 - val_loss: 0.5634 - val_acc: 0.9060\n",
            "Epoch 33/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0760 - acc: 0.9719 - val_loss: 0.5697 - val_acc: 0.9057\n",
            "Epoch 34/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0756 - acc: 0.9720 - val_loss: 0.5714 - val_acc: 0.9049\n",
            "Epoch 35/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0751 - acc: 0.9721 - val_loss: 0.5767 - val_acc: 0.9051\n",
            "Epoch 36/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0750 - acc: 0.9721 - val_loss: 0.5799 - val_acc: 0.9054\n",
            "Epoch 37/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0744 - acc: 0.9722 - val_loss: 0.5836 - val_acc: 0.9051\n",
            "Epoch 38/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0735 - acc: 0.9724 - val_loss: 0.5851 - val_acc: 0.9049\n",
            "Epoch 39/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0732 - acc: 0.9724 - val_loss: 0.5872 - val_acc: 0.9046\n",
            "Epoch 40/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0727 - acc: 0.9726 - val_loss: 0.5923 - val_acc: 0.9051\n",
            "Epoch 41/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0729 - acc: 0.9725 - val_loss: 0.5961 - val_acc: 0.9046\n",
            "Epoch 42/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0728 - acc: 0.9725 - val_loss: 0.6006 - val_acc: 0.9045\n",
            "Epoch 43/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0729 - acc: 0.9724 - val_loss: 0.6027 - val_acc: 0.9047\n",
            "Epoch 44/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0717 - acc: 0.9727 - val_loss: 0.6036 - val_acc: 0.9046\n",
            "Epoch 45/50\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0714 - acc: 0.9728 - val_loss: 0.6100 - val_acc: 0.9039\n",
            "Epoch 46/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0730 - acc: 0.9723 - val_loss: 0.6052 - val_acc: 0.9051\n",
            "Epoch 47/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0719 - acc: 0.9725 - val_loss: 0.6132 - val_acc: 0.9044\n",
            "Epoch 48/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0705 - acc: 0.9730 - val_loss: 0.6152 - val_acc: 0.9047\n",
            "Epoch 49/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0695 - acc: 0.9732 - val_loss: 0.6174 - val_acc: 0.9044\n",
            "Epoch 50/50\n",
            "157/157 [==============================] - 23s 147ms/step - loss: 0.0691 - acc: 0.9732 - val_loss: 0.6179 - val_acc: 0.9046\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNQ9W93B0LUx"
      },
      "source": [
        "입력값은 똑같이 적용하였으며<br>\n",
        "학습결과\n",
        "- overfit이 되지않는 모델의 경우는 정상적으로 우하향하는 그래프의 모습을 보이며<br>train loss, validation loss 의 차이가 크지않다.\n",
        "- overfit이 된 모델은 대략 epoch이 8 이후부터 이상점이 드러나며<br>\n",
        "어느 기점으로 validation loss의 값이 증가하는 모습을 볼 수 있다.\n",
        "\n",
        "라는 결과를 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 검증\n"
      ],
      "metadata": {
        "id": "mv2-aMcEvAs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################<<<nomal>>>##################################################################\n",
        "\n",
        "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
        "encoder_model.summary()\n",
        "\n",
        "# 이전 time step의 hidden state를 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 cell state를 저장하는 텐서\n",
        "decoder_state_input_c = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_embadding_val = decoder_embadding(decoder_inputs)\n",
        "\n",
        "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
        "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
        "decoder_outputs_val, state_h, state_c = decoder_lstm(decoder_embadding_val, initial_state = decoder_states_inputs)\n",
        "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
        "decoder_states_val = [state_h, state_c]\n",
        "\n",
        "decoder_outputs_val = decoder_softmax_layer(decoder_outputs_val)\n",
        "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs_val] + decoder_states_val, name=\"teaching_model\")\n",
        "decoder_model.summary()\n",
        "\n",
        "###################################################<<<overfit>>>##################################################################\n",
        "\n",
        "overfit_encoder_model = Model(inputs = overfit_encoder_inputs, outputs = overfit_encoder_states)\n",
        "overfit_encoder_model.summary()\n",
        "\n",
        "# 이전 time step의 hidden state를 저장하는 텐서\n",
        "overfit_decoder_state_input_h = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 cell state를 저장하는 텐서\n",
        "overfit_decoder_state_input_c = Input(shape=(HIDDEN_STATE_NUM,))\n",
        "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
        "overfit_decoder_states_inputs = [overfit_decoder_state_input_h, overfit_decoder_state_input_c]\n",
        "\n",
        "overfit_decoder_embadding_val = overfit_decoder_embadding(overfit_decoder_inputs)\n",
        "\n",
        "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
        "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
        "overfit_decoder_outputs_val, overfit_state_h, overfit_state_c = overfit_decoder_lstm(overfit_decoder_embadding_val, initial_state = overfit_decoder_states_inputs)\n",
        "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
        "overfit_decoder_states_val = [overfit_state_h, overfit_state_c]\n",
        "\n",
        "overfit_decoder_outputs_val = overfit_decoder_softmax_layer(overfit_decoder_outputs_val)\n",
        "overfit_decoder_model = Model(inputs=[overfit_decoder_inputs] + overfit_decoder_states_inputs, outputs=[overfit_decoder_outputs_val] + overfit_decoder_states_val, name=\"overfit_teaching_model\")\n",
        "overfit_decoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEZRY1YZvA39",
        "outputId": "927a650c-b59a-4cde-e060-7f212e803ed9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         2132224   \n",
            "                                                                 \n",
            " masking (Masking)           (None, None, 256)         0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 [(None, 256),             525312    \n",
            "                              (None, 256),                       \n",
            "                              (None, 256)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,657,536\n",
            "Trainable params: 2,657,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"teaching_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    3763712     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  525312      ['embedding_1[1][0]',            \n",
            "                                 (None, 256),                     'input_5[0][0]',                \n",
            "                                 (None, 256)]                     'input_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 14702)  3778414     ['lstm_1[1][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,067,438\n",
            "Trainable params: 8,067,438\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         2132224   \n",
            "                                                                 \n",
            " masking_2 (Masking)         (None, None, 256)         0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               [(None, 256),             525312    \n",
            "                              (None, 256),                       \n",
            "                              (None, 256)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,657,536\n",
            "Trainable params: 2,657,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"overfit_teaching_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 256)    3763712     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " input_7 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['embedding_3[1][0]',            \n",
            "                                 (None, 256),                     'input_7[0][0]',                \n",
            "                                 (None, 256)]                     'input_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 14702)  3778414     ['lstm_3[1][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,067,438\n",
            "Trainable params: 8,067,438\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습이 과적합된 모델 혹은 아닌모델 두 경우의 teaching forcing을 구현하였다."
      ],
      "metadata": {
        "id": "6_PrcpfDvBDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng2idx = eng_tokenizer.word_index\n",
        "fra2idx = fra_tokenizer.word_index\n",
        "idx2eng = eng_tokenizer.index_word\n",
        "idx2fra = fra_tokenizer.index_word"
      ],
      "metadata": {
        "id": "m1Soi1Am0_wy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 -> 인덱스, 인덱스 -> 단어에 해당하는 데이터 생성"
      ],
      "metadata": {
        "id": "9Q_oqPgi0_5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = fra2idx[SOS_TOKEN]\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    # stop_condition이 True가 될 때까지 루프 반복\n",
        "    while not stop_condition:\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # 예측 결과를 문자로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = idx2fra[sampled_token_index]\n",
        "\n",
        "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
        "        decoded_sentence += \" \" + sampled_char\n",
        "\n",
        "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "        if (sampled_char == EOS_TOKEN or\n",
        "           len(decoded_sentence) > fit_kwargs[\"epochs\"]):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "wPvlKpzY1B5I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "예제에 있던 코드를 가져왔으며 ont-hot vector에 해당하는 부분을 입/출력값에 맞게 조정하였다.<br>\n",
        "이번에는 과적합 여부에 관련된 모델의 출력값을 비교가 필요하므로<br>\n",
        "각 모델의 조건에 따라 결과값을 출력하는 함수로 변경하였다.<br>"
      ],
      "metadata": {
        "id": "EXgwX_Ff1CEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 15\n",
        "input_seq = encoder_input_train[sample: sample + 1]\n",
        "\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"입력문장 :\", \"\".join([\n",
        "        idx2eng[encoded_word] + \" \"\n",
        "        for encoded_word in encoder_input_train[sample]\n",
        "        if encoded_word != 0\n",
        "    ])\n",
        ")\n",
        "print(\"정답문장 :\",  \"\".join([\n",
        "      idx2fra[encoded_word] + \" \"\n",
        "      for encoded_word in decoder_input_train[sample]\n",
        "      if encoded_word != 0 and encoded_word != fra2idx[SOS_TOKEN] and encoded_word != fra2idx[EOS_TOKEN]\n",
        "  ])\n",
        ")\n",
        "\n",
        "print(\"과적합이 안된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, encoder_model, decoder_model)[1:-5])\n",
        "print(\"과적합된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, overfit_encoder_model, overfit_decoder_model)[1:-5])\n",
        "\n",
        "print(\"-\" * 35)\n",
        "\n",
        "input_seq = encoder_input_test[sample: sample + 1]\n",
        "\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"입력문장 :\", \"\".join([\n",
        "        idx2eng[encoded_word] + \" \"\n",
        "        for encoded_word in encoder_input_test[sample]\n",
        "        if encoded_word != 0\n",
        "    ])\n",
        ")\n",
        "print(\"정답문장 :\",  \"\".join([\n",
        "      idx2fra[encoded_word] + \" \"\n",
        "      for encoded_word in decoder_input_test[sample]\n",
        "      if encoded_word != 0 and encoded_word != fra2idx[SOS_TOKEN] and encoded_word != fra2idx[EOS_TOKEN]\n",
        "  ])\n",
        ")\n",
        "\n",
        "print(\"과적합이 안된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, encoder_model, decoder_model)[1:-5])\n",
        "print(\"과적합된 모델\")\n",
        "print(\"번역문장 :\",decode_sequence(input_seq, overfit_encoder_model, overfit_decoder_model)[1:-5])\n",
        "\n",
        "print(\"-\" * 35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5eU9KvT1XZl",
        "outputId": "ca76775f-16db-4e32-f729-2ac25695031a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "입력문장 : tom ' s amazing . \n",
            "정답문장 : tom est incroyable . \n",
            "과적합이 안된 모델\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 337ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "번역문장 : tom est en train de elle . \n",
            "과적합된 모델\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 335ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "번역문장 : tom est incroyable . \n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "입력문장 : she was now out of danger . \n",
            "정답문장 : elle était dès lors hors de danger . \n",
            "과적합이 안된 모델\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "번역문장 : elle est très bon en train de l ' argent . \n",
            "과적합된 모델\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "번역문장 : elle était moins en danger . \n",
            "-----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E7-ju2Gp1Xko"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsx8OVYGnM-s"
      },
      "source": [
        "### 회고\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ngdabGo3j4l"
      },
      "source": [
        "- 학습된 데이터가 전체적으로 높은 정확도를 보여 미리 학습된 데이터의 중요성을 알게되었다.\n",
        "- 항상 학습된 데이터가 높은 정확도를 나타내는 것이 아니라 하이퍼파라미터의 튜닝에 따라 결과가 달라질 수 있다는 것을 알게 되었다.\n",
        "\n",
        "※ 이번 레포트는 양희성님의 모델구조의 조언으로 작성되었음을 알려드립니다.<br>\n",
        "   희성님에게 감사하다는 글을 남기며 이만 글을 마치겠습니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}