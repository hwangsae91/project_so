{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangsae91/project_so/blob/master/exploration/221007/%5BExp_07%5D20221007.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "302ZdfCKLlhI",
        "outputId": "a87cae63-636a-4abd-db90-2d081cf75bbc"
      },
      "outputs": [],
      "source": [
        "# # google colab전용\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujMb2MtyLrVQ"
      },
      "source": [
        "# exploration 7번째 과제</br>\n",
        "@ 황한용(3기/쏘카)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdrfR8Y3L0ry"
      },
      "source": [
        "## 라이브러리 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LWLP8EFSLqqp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import konlpy\n",
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "from konlpy.tag import Mecab\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qC6hZWKL7x3"
      },
      "source": [
        "## 상수선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XcIjEEgnMD75"
      },
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "Install MeCab in order to use it: http://konlpy.org/en/latest/install/",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\konlpy\\tag\\_mecab.py:77\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagger \u001b[39m=\u001b[39m Tagger(\u001b[39m'\u001b[39m\u001b[39m-d \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m dicpath)\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagset \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mread_json(\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/data/tagset/mecab.json\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m utils\u001b[39m.\u001b[39minstallpath)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Tagger' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m MAX_NUM_WORDS \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m \u001b[39m# 최대 사전 단어갯수\u001b[39;00m\n\u001b[0;32m      4\u001b[0m FEATURE_DATA \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m tokenizer \u001b[39m=\u001b[39m Mecab()\n",
            "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\konlpy\\tag\\_mecab.py:82\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe MeCab dictionary does not exist at \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Is the dictionary correctly installed?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMecab(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m/some/dic/path\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m dicpath)\n\u001b[0;32m     81\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInstall MeCab in order to use it: http://konlpy.org/en/latest/install/\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mException\u001b[0m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"
          ]
        }
      ],
      "source": [
        "DATA_PATH = \"/\".join([str(Path(\"./\").parent.resolve().parent.resolve().parent.resolve().absolute()).replace(\"\\\\\",\"/\"),\"data\",\"sentiment_classification\"]) # 데이터 기본경로\n",
        "STEP_WORDS = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다'] # 불용어\n",
        "MAX_NUM_WORDS = 10000 # 최대 사전 단어갯수\n",
        "FEATURE_DATA = [\"document\"]\n",
        "tokenizer = Mecab()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(\n",
        "    train_data:pd.DataFrame\n",
        "    , test_data:pd.DataFrame\n",
        "    , num_words:int=MAX_NUM_WORDS\n",
        ") -> Tuple[\n",
        "            np.ndarray\n",
        "            , np.ndarray\n",
        "            , np.ndarray\n",
        "            , np.ndarray\n",
        "            , Dict[int,str]\n",
        "        ]:\n",
        "    \"\"\"\n",
        "    다음과 같은 전처리 후\n",
        "    `train data(feature, target)`, `test data(feature, target)`, `단어사전`\n",
        "    을 리턴한다.\n",
        "\n",
        "    - 데이터의 중복 제거\n",
        "    - `NaN` 결측치 제거\n",
        "    - 한국어 토크나이저로 토큰화\n",
        "    - 불용어(`stop_words`) 제거\n",
        "    - 사전`word_to_index` 구성\n",
        "    - 텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_data : DataFrame\n",
        "        학습 데이터\n",
        "    train_data : DataFrame\n",
        "        테스트 데이터\n",
        "    num_words : int, default = `MAX_NUM_WORDS`\n",
        "        단어사전의 최대 단어 갯수\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_train : ndarray\n",
        "        train feature data\n",
        "    y_train : ndarray\n",
        "        train target data\n",
        "    X_test : ndarray\n",
        "        test feature data\n",
        "    y_test : ndarray\n",
        "        test target data\n",
        "    word_to_index : dict\n",
        "        단어사전\n",
        "    \"\"\"\n",
        "    # 1.\n",
        "    train_data.drop_duplicates(subset=FEATURE_DATA, inplace=True)\n",
        "    test_data.drop_duplicates(subset=FEATURE_DATA, inplace=True)\n",
        "\n",
        "    # 2.\n",
        "    train_data.dropna(how='any', inplace=True)\n",
        "    test_data.dropna(how='any', inplace=True)\n",
        "\n",
        "    tokenizer.morphs(train_data[FEATURE_DATA])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5DV1kd8MPes"
      },
      "source": [
        "## 메인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwkvcfr7MPsm"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_table(DATA_PATH + \"/ratings_train.txt\")\n",
        "test_data = pd.read_table(DATA_PATH + \"/ratings_test.txt\")\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "데이터에 대한 설명은 아래와 같다.</br>\n",
        "- id: The review id, provieded by Naver\n",
        "- document: The actual review\n",
        "- label: The sentiment class of the review. (0: negative, 1: positive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOCGZoyQ9QB5/gXXMw1RTO8",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
