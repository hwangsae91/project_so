{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangsae91/project_so/blob/master/exploration/221011/%5BExp_08%5D20221011.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "302ZdfCKLlhI",
        "outputId": "23a2eefe-a59d-4103-fbf0-b513212d9352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# google colab전용\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mecab 설치를 위한 환경구성\n",
        "import os\n",
        "\n",
        "# install konlpy, jdk, JPype\n",
        "!pip install konlpy > /dev/null 2>&1\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null 2>&1\n",
        "!pip3 install JPype1-py3 > /dev/null 2>&1\n",
        "\n",
        "# install mecab-ko\n",
        "os.chdir('/tmp/')\n",
        "!curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz > /dev/null 2>&1\n",
        "!tar zxfv mecab-0.996-ko-0.9.2.tar.gz > /dev/null 2>&1\n",
        "os.chdir('/tmp/mecab-0.996-ko-0.9.2')\n",
        "!./configure > /dev/null 2>&1\n",
        "!make > /dev/null 2>&1\n",
        "!make check > /dev/null 2>&1\n",
        "!make install > /dev/null 2>&1\n",
        "\n",
        "# install mecab-ko-dic\n",
        "!apt-get install automake > /dev/null 2>&1\n",
        "os.chdir('/tmp')\n",
        "!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz > /dev/null 2>&1\n",
        "!tar -zxvf mecab-ko-dic-2.1.1-20180720.tar.gz > /dev/null 2>&1\n",
        "os.chdir('/tmp/mecab-ko-dic-2.1.1-20180720')\n",
        "!./autogen.sh > /dev/null 2>&1\n",
        "!./configure > /dev/null 2>&1\n",
        "!make > /dev/null 2>&1\n",
        "!make install > /dev/null 2>&1\n",
        "\n",
        "# install mecab-python\n",
        "os.chdir('/content')\n",
        "!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git > /dev/null 2>&1\n",
        "os.chdir('/content/mecab-python-0.996')\n",
        "!python3 setup.py build > /dev/null 2>&1\n",
        "!python3 setup.py install > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "aUd9iIbhAgk5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "※ 환경설정의 출력은 명령어를 통해 출력되지 않게 하였다."
      ],
      "metadata": {
        "id": "RbKTjedWnAKZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujMb2MtyLrVQ"
      },
      "source": [
        "# exploration 8번째 과제</br>\n",
        "@ 황한용(3기/쏘카)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdrfR8Y3L0ry"
      },
      "source": [
        "## 라이브러리 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LWLP8EFSLqqp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import Counter\n",
        "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
        "from konlpy.tag import Mecab\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from typing import Dict, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qC6hZWKL7x3"
      },
      "source": [
        "## 상수선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XcIjEEgnMD75"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/data/sentiment_classification\" # 데이터 기본경로\n",
        "DATA_PATH  = BASE_PATH + \"/data\" # 데이터 기본경로\n",
        "MODEL_PATH = BASE_PATH + \"/model/word2vec_ko.model\" # 학습된 한국어 단어모델\n",
        "STEP_WORDS = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다'] # 불용어\n",
        "BASE_DICTONARY = ['<PAD>', '<BOS>', '<UNK>','<UNUSED>'] # 단어의 기본구성\n",
        "FEATURE_DATA = [\"document\"] # feature\n",
        "MAX_NUM_WORDS = 10000 # 최대 사전 단어갯수\n",
        "WORD_VECTOR_DIM = 100 # 단어 백터 차원수\n",
        "pad_seq_kwargs = {\n",
        "    \"value\":None # 추후 추가예정\n",
        "    , \"padding\":\"post\"\n",
        "    , \"maxlen\":None # 추후 추가예정\n",
        "}\n",
        "TRAIN_TEST_SPLIT_KWARGS = {\n",
        "    \"test_size\":0.4, \"random_state\":2022\n",
        "}\n",
        "fit_kwargs = {\n",
        "    \"epochs\":10 # epoch 횟수\n",
        "    ,\"validation_data\": None # 추후 추가예정\n",
        "    , \"shuffle\" : True # epoch당 셔플을 할지의 여부\n",
        "    , \"verbose\":1\n",
        "}\n",
        "tokenizer = Mecab() # tokenize 인스턴스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQO-EqdI8459"
      },
      "source": [
        "## 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9WiQ4qfK8459"
      },
      "outputs": [],
      "source": [
        "def load_data(\n",
        "    train_data:pd.DataFrame\n",
        "    , test_data:pd.DataFrame\n",
        "    , step_words:list=STEP_WORDS\n",
        "    , num_words:int=MAX_NUM_WORDS\n",
        ") -> Tuple[\n",
        "            np.ndarray\n",
        "            , np.ndarray\n",
        "            , np.ndarray\n",
        "            , np.ndarray\n",
        "            , Dict[int,str]\n",
        "           , Dict[str,int]\n",
        "        ]:\n",
        "    \"\"\"\n",
        "    다음과 같은 전처리 후\n",
        "    `train data(feature, target)`, `test data(feature, target)`, `단어사전(str기반, index기반)`\n",
        "    을 리턴한다.\n",
        "\n",
        "    - 데이터의 중복 제거\n",
        "    - `NaN` 결측치 제거\n",
        "    - 한국어 토크나이저로 토큰화\n",
        "    - 불용어(`stop_words`) 제거\n",
        "    - 사전`word_to_index` 구성\n",
        "    - 텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_data : DataFrame\n",
        "        학습 데이터\n",
        "    train_data : DataFrame\n",
        "        테스트 데이터\n",
        "    step_words : list, default = `MAX_NUM_WORDS`\n",
        "        불용어\n",
        "    num_words : int, default = `MAX_NUM_WORDS`\n",
        "        단어사전의 최대 단어 갯수\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_train : ndarray\n",
        "        train feature data\n",
        "    y_train : ndarray\n",
        "        train target data\n",
        "    X_test : ndarray\n",
        "        test feature data\n",
        "    y_test : ndarray\n",
        "        test target data\n",
        "    word_to_index : dict\n",
        "        단어사전\n",
        "    index_to_word : dict\n",
        "        인덱스 기반 단어사전\n",
        "    \"\"\"\n",
        "    # 1.\n",
        "    train_data.drop_duplicates(subset=FEATURE_DATA, inplace=True)\n",
        "    test_data.drop_duplicates(subset=FEATURE_DATA, inplace=True)\n",
        "\n",
        "    # 2.\n",
        "    train_data.dropna(how='any', inplace=True)\n",
        "    test_data.dropna(how='any', inplace=True)\n",
        "\n",
        "    # 3.\n",
        "    X_train = [\n",
        "        [word for word in tokenizer.morphs(sentence) if not word in step_words]\n",
        "        for sentence in train_data['document']\n",
        "    ]\n",
        "\n",
        "    X_test = [\n",
        "        [word for word in tokenizer.morphs(sentence) if not word in step_words]\n",
        "        for sentence in test_data['document']\n",
        "    ]\n",
        "    \n",
        "    # 4.\n",
        "    words = np.concatenate(X_train).tolist()\n",
        "    counter = Counter(words)\n",
        "    counter = counter.most_common(num_words - len(BASE_DICTONARY))\n",
        "    vocab = BASE_DICTONARY + [key for key, _ in counter]\n",
        "\n",
        "    # 5.\n",
        "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
        "    index_to_word = {index:word for word, index in word_to_index.items()}\n",
        "        \n",
        "    def wordlist_to_indexlist(wordlist):\n",
        "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
        "        \n",
        "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
        "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
        "\n",
        "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index, index_to_word\n",
        "\n",
        "####################################### 제공 함수(사용안함) #######################################\n",
        "\n",
        "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
        "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
        "def get_encoded_sentence(sentence, word_to_index):\n",
        "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
        "\n",
        "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
        "def get_encoded_sentences(sentences, word_to_index):\n",
        "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
        "\n",
        "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
        "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
        "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
        "\n",
        "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
        "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
        "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5DV1kd8MPes"
      },
      "source": [
        "## 메인"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMDB 데이터셋"
      ],
      "metadata": {
        "id": "B7XCWajZ0XIj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iwkvcfr7MPsm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "5481e823-184e-4b77-b047-e10732e51564"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79b8e175-fd58-4849-9372-ca44989498f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79b8e175-fd58-4849-9372-ca44989498f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-79b8e175-fd58-4849-9372-ca44989498f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-79b8e175-fd58-4849-9372-ca44989498f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_data = pd.read_table(DATA_PATH + \"/ratings_train.txt\")\n",
        "test_data = pd.read_table(DATA_PATH + \"/ratings_test.txt\")\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXm0KjWe845_"
      },
      "source": [
        "데이터에 대한 설명은 아래와 같다.</br>\n",
        "- id: The review id, provieded by Naver\n",
        "- document: The actual review\n",
        "- label: The sentiment class of the review. (0: negative, 1: positive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "971TB3kw845_",
        "outputId": "6cfb1113-76c0-446c-c486-7e1f768fb60e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train len:146182\n",
            "X_test len:49157\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, X_test, y_test, word_to_index, index_to_word = load_data(train_data, test_data)\n",
        "\n",
        "print(f\"X_train len:{len(X_train)}\")\n",
        "print(f\"X_test len:{len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMm1Q3DE846A"
      },
      "source": [
        "학습데이터와 테스트데이터를 로드"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = np.array(list(map(len,list(X_train) + list(X_test))))\n",
        "# 문장길이의 평균값, 최대값, 표준편차를 계산 \n",
        "print('문장길이 평균 : ', np.mean(num_tokens))\n",
        "print('문장길이 최대 : ', np.max(num_tokens))\n",
        "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
        "\n",
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "maxlen = int(max_tokens)\n",
        "print('pad_sequences maxlen : ', maxlen)\n",
        "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))\n",
        "\n",
        "pad_seq_kwargs.update({\n",
        "    \"value\":word_to_index[\"<PAD>\"]\n",
        "    , \"maxlen\":maxlen\n",
        "})# padding 설정"
      ],
      "metadata": {
        "id": "3ZASR2F_fugN",
        "outputId": "43002cd6-93b7-4e1a-b405-8fe9bc09de88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장길이 평균 :  15.969376315021577\n",
            "문장길이 최대 :  116\n",
            "문장길이 표준편차 :  12.843535456326455\n",
            "pad_sequences maxlen :  41\n",
            "전체 문장의 0.9342988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 최대 길이를 `maxlen`으로 설정하였고 값은 (평균 + 2*표준편차)로 하여<br>\n",
        "대부분의 문장이 학습이 가능하도록 설정하였다.  "
      ],
      "metadata": {
        "id": "wRHCeqlem4v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, **pad_seq_kwargs)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, **pad_seq_kwargs)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, **TRAIN_TEST_SPLIT_KWARGS)\n",
        "fit_kwargs[\"validation_data\"] = (X_val, y_val)"
      ],
      "metadata": {
        "id": "HbFIKeNV_L8U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "과적합 방지를 위해 학습과 점증을 2:8비율로 나누었다. "
      ],
      "metadata": {
        "id": "cQlTE3xms-k0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 한국어 Word2Vec 임베딩"
      ],
      "metadata": {
        "id": "Iu9puDTo_MRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = Word2VecKeyedVectors.load(MODEL_PATH).wv\n",
        "\n",
        "# 임의의 가중치 적용\n",
        "embedding_matrix = np.random.rand(MAX_NUM_WORDS, WORD_VECTOR_DIM)\n",
        "\n",
        "# 이미 학습된 단어가 있는 경우 학습된 백터를 넣는다. \n",
        "for i, w in list(index_to_word.items())[4:]:\n",
        "    if w in word_vectors:\n",
        "        embedding_matrix[i] = word_vectors[w]"
      ],
      "metadata": {
        "id": "2qbNBHBm0LLy",
        "outputId": "0fcf3ff7-0307-432f-e5d5-b84d8bd4e2c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.99275217e-01  4.62225481e-01  7.48922154e-01 ...  9.06083423e-01\n",
            "   6.24763435e-02  6.36164994e-01]\n",
            " [ 2.87175828e-01  3.77738968e-01  3.96647119e-01 ...  3.34706503e-03\n",
            "   6.49053891e-01  8.45156999e-01]\n",
            " [ 3.28312285e-01  6.11347263e-01  1.87399977e-01 ...  9.37818915e-01\n",
            "   1.87059165e-02  6.44619538e-01]\n",
            " ...\n",
            " [ 8.07251632e-02  8.92280787e-02  3.37170884e-02 ... -5.61643690e-02\n",
            "   1.13671413e-03  5.08995205e-02]\n",
            " [-8.71433131e-03  6.62038382e-03 -8.34809095e-02 ... -8.07309225e-02\n",
            "   1.48725137e-02  7.11738542e-02]\n",
            " [ 6.82061553e-01 -3.08079198e-02 -1.15238059e+00 ...  3.06192100e-01\n",
            "   6.42204165e-01  3.65547091e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본에 학습된 모델을 로드 뒤, 같은 단어장에 가중치를 랜덤으로 적용 후,<br>\n",
        "단어장 안에 학습된 단어가 존제하면 백터의 값을 대입한다."
      ],
      "metadata": {
        "id": "XNQ9W93B0LUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_list = []\n",
        "\n",
        "# max pooling만 존제하는 model 설계\n",
        "model_maxpool = tf.keras.Sequential(name = \"maxpool_model\")\n",
        "model_maxpool.add(tf.keras.layers.Embedding(MAX_NUM_WORDS, WORD_VECTOR_DIM, input_shape=(None,)))\n",
        "model_maxpool.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model_maxpool.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model_maxpool.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_list.append(model_maxpool)"
      ],
      "metadata": {
        "id": "3Y-xHhXejA5P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8sbpd_8bjBGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN 1d model 설계\n",
        "model_CNN = tf.keras.Sequential(name = \"CNN_model\")\n",
        "model_CNN.add(tf.keras.layers.Embedding(MAX_NUM_WORDS, WORD_VECTOR_DIM, input_shape=(None,)))\n",
        "model_CNN.add(tf.keras.layers.Dropout(rate=0.2))\n",
        "model_CNN.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
        "model_CNN.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model_CNN.add(tf.keras.layers.Dropout(rate=0.2))\n",
        "model_CNN.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
        "model_CNN.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model_CNN.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model_CNN.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
        "\n",
        "model_list.append(model_CNN)"
      ],
      "metadata": {
        "id": "KuuY-SGVmbWl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HliFm2DDmbiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN model 설계\n",
        "model_RNN = tf.keras.Sequential(name = \"RNN_model\")\n",
        "model_RNN.add(tf.keras.layers.Embedding(MAX_NUM_WORDS, WORD_VECTOR_DIM, input_shape=(None,)))\n",
        "model_RNN.add(tf.keras.layers.LSTM(8))\n",
        "model_RNN.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model_RNN.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
        "\n",
        "model_list.append(model_RNN)"
      ],
      "metadata": {
        "id": "U4R0Zm-mlCzH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MWLtgaTelC60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in model_list:\n",
        "    print(f\"{model.name} start\")\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "                        learning_rate=0.00005 # running rate\n",
        "                )\n",
        "                ,loss='binary_crossentropy'\n",
        "                ,metrics=['accuracy'])\n",
        "\n",
        "    history_dict = model.fit(\n",
        "                        X_train\n",
        "                        , y_train\n",
        "                        , **fit_kwargs\n",
        "                    ).history\n",
        "\n",
        "    acc = history_dict['accuracy']\n",
        "    val_acc = history_dict['val_accuracy']\n",
        "    loss = history_dict['loss']\n",
        "    val_loss = history_dict['val_loss']\n",
        "\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    # \"bo\"는 \"파란색 점\"입니다\n",
        "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "    # b는 \"파란 실선\"입니다\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    plt.clf()   # 그림을 초기화합니다\n",
        "\n",
        "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # 테스트셋을 통한 모델 평가\n",
        "    results = model.evaluate(X_test, y_test, verbose=2)\n",
        "    print(results)\n",
        "\n",
        "    print(f\"{model.name} end\\n\")"
      ],
      "metadata": {
        "id": "BQmNPZNVoPmW",
        "outputId": "2bc5fabb-6244-45ba-be84-60749c86ea84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maxpool_model start\n",
            "Epoch 1/10\n",
            " 669/2741 [======>.......................] - ETA: 6s - loss: 0.6883 - accuracy: 0.5504"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ea61fdf31a5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0mX_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                         \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                         \u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                     ).history\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4318q_hboPvg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}